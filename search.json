[
  {
    "objectID": "posts/InstallingTensorflow/index.html",
    "href": "posts/InstallingTensorflow/index.html",
    "title": "Installing Tensorflow",
    "section": "",
    "text": "Getting tensorflow to use hardware acceleration is challenging. There’s plenty of tutorials to show you how to do it, but I’ve found that most of them lack sufficient detail, aren’t targeted towards what I’m doing (they’re made for super computing clusters or fine-grain control, or something), or are outdated. The ML field is moving fast right now, and things get outdated fast.\nWell, here’s yet another blog post. This is what I did to get tensorflow to connect to my GPU, maybe it’ll help you.\n\nHardware\nFirst off, some hardware. Tensorflow seems to work best on NVIDIA GPUs. I’m told it’s possible to get it to work on an AMD GPU, but the one time I tried, I couldn’t get it to work. You can see this for-pay article for more guidance, in that case.\nHere’s my screenfetch: \n\n\nGuide\nWhen I set it up, I followed this blog post. It contains a lot of good detail, so instead of repeating everything it says, just follow it, and I’ll tell you what I had to change to get it to work.\nWhen installing drivers, I installed nvidia-driver-550, and then had to restart to activate it, which isn’t in the tutorial. According to this page, anything &gt;=520.;61.05 should work. Later on, while trying to debug the later steps, I ended up installing 525 as well, however, I don’t think that actually fixed anything.\nI also had to install the cuda-toolkit package (sudo apt install cuda-toolkit) to get it to run, which isn’t mentioned in the post.\nInstead of the versions specified in the post, I used these versions specifically:\n\npython3.11\ncuDNN==8.8\ncudatoolkit==11.8\ntensorflow==2.14\n\nNOTE: The cudatoolkit version is the version of the Python package; upon running nvidia-smi, my driver says it’s CUDA version 12.2. This didn’t seem to cause problems.\nI used this table from the post to align all the versions. I had a bit of a problem, because as of 5/22/24, cuDNN==8.7 doesn’t exist on conda for some reason, and cudatoolkit&gt;=12 was not released on conda yet, so had to use python3.11 and guess at some of the package versions, but the ones I used worked out.\nAfter following all the steps in the post and running the benchmark test mentioned, it threw and error with a bunch of warnings and errors and traceback and other stuff. After digging through it and some research, I found this stackoverflow post that advised running this command:\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/cuda\nwhich worked. You may want to follow the steps in the article to find your specific cuda installation path, but it’s likely /usr/lib/cuda. You’ll also likely want to put that line in your ~/.bashrc file, so it will work in other terminals as well.\nMy benchmarks were:\n\nGPU: 17.934 seconds\nCPU: 219.757 seconds\n\nA 12.25x speedup! Not bad!\n\n\nPhoto courtesy of Coding for Entrepreneurs"
  },
  {
    "objectID": "posts/DynamicStateMachines/index.html",
    "href": "posts/DynamicStateMachines/index.html",
    "title": "Dynamic State Machine",
    "section": "",
    "text": "A bit ago I was writing a project to automate the workflow for my job. What it quickly evolved into was a textual TUI that was just a customizeable text editor, and what amounted to an automated flowchart. I had a text box I could add text to so I could quickly jump through the flowchart and have it do things for me as I reached certain steps, keep track of what I was doing, and follow preset lines of logic depending on what inputs I gave it. Naturally, it started off as a bunch of if & match statements. I kept a flowchart in sync with it just for documentation, but eventually it became unmaintainable. I would make minor tweaks to it almost daily, and keeping the flowchart in pace with it was a huge headache.\nOf course my next step is to go see what other people have done. I found transitions and python-statemachine, both very good, mature projects. What they don’t do however (as far as I could tell), is let you easily decide what state to go to next based on parameters given to the advancement function. It’s possible in at least one of them (I don’t remember which), but it was clunky, frustating, and less elegant that what I was already doing.\nSo I made my own.\nI couldn’t think up a witty name, so it’s simply called DynamicStateMachine. I won’t go into details on exactly how it works (see the README for that), but it basically lets you implement a state machine (flowchart) where you can use complex logic to decide which state to go to next. You define a bunch of states, which have values of any type, then you define an initial state, and then the transitions of how you go from one state to the next. Each state goes to either another state, or a function which returns which state (or other function) to go to next. You can also add before_* and after_* functions to allow states to have side effects. Then any parameters passed to the advancement function (.next()) get passed onto both the transition functions and the side effect functions.\nIt works pretty well, and the best part is, it auto-generates flow charts for you, which is a huge help not only for documentation, but also debugging. If you generate a flowchart and there’s steps not going anywhere, or by themselves, you know you forgot to connect something.\nIt’s still in beta, and it needs more tests, but it works for my current implementation!"
  },
  {
    "objectID": "posts/SquarePacking/index.html",
    "href": "posts/SquarePacking/index.html",
    "title": "Square Packing",
    "section": "",
    "text": "My senior project is to attempt to use a reinforcement learning algorithm based on a Deep Deterministic Policy Gradient model for use with a continuous observation and action space, in order to solve the Square Packing in a Square problem for N=11 squares.\nThat was a very intelligent sounding sentence. Let’s break it down:\n\nSquare Packing in a Square\nThe Square Packing in a Square problem is an unsolved problem in mathematics where the goal is to pack N squares with a side length of 1 into another square, while wasting as little space as possible.\nThe technical wording is\n\nWhat is the asymptotic growth rate of wasted space for square packing in a half-integer square?\n\nI’m not going to be solving that. Instead I’m going to attempt to use Reinforcement Learning (RL) to attempt to figure out a better solution for me. I have no idea how well it will work.\nThere are known configurations for N=1-10 squares, but 11 (and some others) are only approximately solved. This tries to find a more optimal configuration for N=11 squares by using RL instead of pure math.\nThe theoretial optimal packing has a side length of 3.789, but the best known is 3.877084\n\n\n\nDDPG\nDDPG model is a kind of actor-critic setup (not technically a model) that allows continuous rather than discrete observation and action spaces. This is important, because I want to find a very precise solution, as opposed to infinitely increasing the discrete resolution of steps the AI can take.\nAn actor-critic setup has 2 models (though the DDPG setup has 4, for training purposes). One model is the actor, which takes in an observation from the enviorment, learns to outputs actions to take in the enviorment, optimized to some reward function. The other model takes in an observation from the enviorment and the action taken by the actor for that observation, and learns to output the reward that that action will get for that observation. The actor model can then use the output of the critic model to train itself.\nIt’s kind of like learning to dance to a song on a stage based on some guy in the back either clapping or yelling “boooo!”. Eventually you can start to understand what the critic is asking for.\nThis is the fundemental equation behind the DDPG algorithm. I understood it at one point, but have since forgot. See the above links for a real explanation.\n\n\n\nImplementation\nBoth the action and observation spaces will have the shape (3*N, ) (N is the number of squares, so 11) where each square has an x, y and rotation values. The actor’s actions simply get added to the current square positions and rotations (since the actor actions can be negative), so the actor can make small adjustments to squeze the squares closer to each other.\nThe reward function hasn’t been developed yet, but it will involve something like steeply punishing overlapping squares, and incentivizing smaller “bounding square” side lengths (like a bounding rectangle, but a square).\nI’ll be using Shapely for the geometry handling:\n\n\nCode\n# This is an example of converting the action space into a shapely object\nMultiPolygon(convert2shapely([(random.uniform(0, space), random.uniform(0, space), random.uniform(1, 2*math.pi)) for i in range(N)], side_len=scale))\n\n\n\n\n\n\n\n\n\nI’ll use Tensorflow for the models, this is my proposed actor network structure:\n\nAnd for the RL enviorment, I’ll use the popular Gymnasium library, specifically my own personal SimpleGym class which is a helpful abstraction on top of gymnasium:"
  },
  {
    "objectID": "posts/GeoDoodle/index.html",
    "href": "posts/GeoDoodle/index.html",
    "title": "GeoDoodle",
    "section": "",
    "text": "I love doodling on graph paper.\nIt’s just something I’ve done for a long time, since I was a kid. It’s something to keep my hands busy while I sit in church or class. It occupies just enough of my mind to keep me entertained, but not too much that I can’t pay attention. It’s perfect.\nAnd over the years, I’ve gotten pretty good, if I don’t say so myself. I have something like 3 or 4 notebooks full of these sorts of patterns:\n    \nBut that’s the problem, isn’t it. They’re in notebooks. I’m not even sure where they are. I don’t look at them or add to them anymore.\n\nHistory\nEver since my very 2nd programming class (the first class was all terminal based), I had the idea for “automated graph paper”. All the lines line up with the dots. It wouldn’t be that difficult to make a program that had a bunch of dots and let you draw lines between them. Back then I had grand ideas of mirroring, selecting, copy/pasting, and repeating lines, but I didn’t have any way to execute them. I had an old, outdated, unsupported (even back then) freeGLUT C++ framework that my class gave me that I hacked together to show an array of dots on the screen, and was barely able to connect lines between them. And then the house of cards that was my code fell apart and I moved onto other things.\nAbout a year later, after starting many other projects, and knowing way, way more, I discovered Python. I realized that my old idea I loved so much was still a cool idea, and I got excited and I ended up scrapping the entire codebase and rewriting the entire thing from scratch in Python (using pygame). I got it roughtly to where it was before, in about 4 hours. It’s pretty cool to see yourself improve in such an obvious way. I then added a bunch more features and made everything a lot nicer (pygame is an excellent API), including adding menus and options and a better repeating system. I eventually got stymied by the GUI though. At that point I hadn’t really done much with GUI’s, and I tried using pygame-gui, which isn’t a bad API, it’s just really not meant for what I wanted. I ended up writing huge, very nasty wrappers around their classes and it just wasn’t worth it. It again, became a house of cards and I saw that.\nA couple months later, I had the idea to use Qt. It was something I’d wanted to learn for a while, but I never really had a project suited to it, until I realized this was perfect. Turns out, Qt is fantastic. There’s a reason it’s so popular. It definitely takes some getting used to, but it’s all very clean, and QtCreator is super handy. Using Qt allowed me to expand even further, and add more features far more intuitively.\nA few months after that, I took a linear algebra course and had the realization that a bunch of the problems I had with repeating patterns and keeping track of coordinates could all be solved by using matrices for coordinates and multiplying by a transformation matrix. I then rewrote the whole codebase (again) to use numpy to represent dots and lines instead. That solved a ton of conceptual problems, and allowed me to add a bunch of features like mirroring and rotating and scaling. It also cleaned a the code up a ton too. That time it wasn’t a total rewrite, because I was able to re-use a lot of the PyQt GUI elements.\nThis is the last working version of GeoDoodle I had in Python \nAll these versions had a few shared, fundemental problems, however:\n\nThe coordinate systems where complicated. Even after I switched to using proper linear algebra, it was still complicated to keep track of, and involved writing my own coordinate system & utilies, essentially.\nDeployment. I love Python to pieces, and it’s the optimal programming language for a lot of things, but it really doesn’t deploy to other platforms very well. I always wanted to show my friends & family my cool program, but I never could, because trying to say “go into the terminal, git clone my repo, install python, and run the main.py file” really isn’t possible for my non-tech savvy mother. I tried a few things like pyqt5deploy and the Qt for Android tutorials, but I couldn’t get them to work\nEfficiency. When a pattern we want to repeat is small enough (meaning that more of them fit on the screen), and has too many lines in it, that means we could be drawing up to hundreds or thousands of lines on the screen, and if I want the user to be able to move around or add lines, depending on how I do it, that could mean they have to update in real-time. This was obviously a problem for freeGLUT (where I had to make raw OpenGL commands, which I didn’t know how to do correctly), and pygame (which isn’t really optimized for that sort of thing), but for PyQt you might not think it would be a problem. It was certainly better than the first 2, but it still got unusably slow under those conditions. I tried optimizing using OpenGL, but that only led down a deeper rabit hole I didn’t really want to go down.\n\n\n\nEnter: JavaScript\nRecently (as of this post) I got an internship writing TypeScript React code. I previously didn’t know Java/TypeScript, so I had a couple week long crash course on JS programming before I started. After I started, I immediately realized the potential JavaScript holds. Not because it’s good or proper (I think JS is terribly janky sometimes), but because it’s not. JS is geared towards writing UIs, which it does well. And part of UIs seems to be inherent jankiness, it seems. Lambda’s galore, strange, optional syntax (semicolons aren’t required, but everyone uses them anyway??), and objects that don’t act like objects in any other language I’d learned. It’s almost as if JS isn’t Object-Oriented, but instead Event-Oriented.\nHowever it has a number of key advantages. Aside from being made for making UIs, like I mentioned, it’s supported everywhere and is made for deployment. Very quickly after learning it I made a couple projects, including Debate-Tracker, and EZRegex. I then realized: my favorite project, my “unique” idea (I haven’t researched if it actually is, and I don’t intend to), GeoDoodle, is perfect for JS. The project is largely UI elements, once the base part is done, I wanted to use it from both my computer and my phone, and SVG is the natural format for the project. Given that HTML integrates seemlessly with SVG, React was perfect.\nSo of course, I started from scratch and rewrote the whole thing again from the ground up in another language. It’s a lot easier the 4th (5th?) time you do it, because you know beforehand what a lot of the problems you’re going to run into are, and how to solve them, and you also know what you need to do in order to generalize things for future features so you don’t end up refactoring your code too many times.\nI quickly got an MVP working (after a weekend of working on it obsessively), and kept adding to it. It now has multiple menus, including a controls menu instead of just using keyboard shortcuts, a help menu with a guided tour (surprisingly easy to make), multiple kinds of mirroring (you’d think it would be simple, but it’s more complex than you think), file saving, either natively to SVG files or to the local storage, basic repeating (is what I’m working on right now), and more. You can check it out at smartycope.github.io/geodoodle. I have it hosted on GitHub pages, and eventually I’ll probably by a URL for it to live at.\nThis time, I think I finally got it right. The code is clean and maintainable enough that I think when I inevitably come back to it in a year or so after I lose intrest, I’ll be able to pick it up again and just add features to it instead of rewriting it again. And the fact that I actually know how to use git now helps with that as well.\nYou can try it here, if you’d like:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Copeland Carter (smartycope) loves his wife, his personal programming projects, Brandon Sanderson novels, rollerblading, and data science (especially really well made graphs)."
  },
  {
    "objectID": "about.html#data-scientist-software-engineer-full-stack-developer",
    "href": "about.html#data-scientist-software-engineer-full-stack-developer",
    "title": "About",
    "section": "",
    "text": "Copeland Carter (smartycope) loves his wife, his personal programming projects, Brandon Sanderson novels, rollerblading, and data science (especially really well made graphs)."
  },
  {
    "objectID": "familyGraphs.html",
    "href": "familyGraphs.html",
    "title": "Thueson Family Graphs",
    "section": "",
    "text": "Code\n# data = gsheet2tbl('https://docs.google.com/spreadsheets/d/1oUMIpE_Elh_CYD7DFbT1vbYxFZNYUDK2QxFqFDk4xII/edit#gid=2008169700') # Original Data\ndata = gsheet2tbl('https://docs.google.com/spreadsheets/d/1oUMIpE_Elh_CYD7DFbT1vbYxFZNYUDK2QxFqFDk4xII/edit?gid=0#gid=0') # Manually cleaned data (it was easier to edit it in the spreadsheet)\nCode\nquestions = colnames(data)\ncolnames(data) = c('timestamp', 'name', 'birthday', 'parents', 'names.indexed', 'ordinances.self', 'ordinances.dead', 'fav.recipe', 'gen', 'tech.savviness', 'injuries', 'num.trips', 'trips', 'num.pets', 'pets', 'fav.part', 'suggested.activity', 'suggested.reunion.location', 'jobs', 'most.told.story', 'oldest', 'rating', 'fav.game', 'secret', 'best.advice', 'worst.advice', 'coming')\ndata$age = floor((today() - mdy(data$birthday)) / 365) |&gt; as.integer()\ndata$first.name.raw = str_split(data$name, pattern=' ') |&gt; map(~ .x[[1]]) |&gt; as.character()\nCode\n# Function to make names unique\nmake_names_unique &lt;- function(names) {\n    # Create a data frame to store the unique names and their counts\n    name_counts &lt;- data.frame(names, count = ave(names, names, FUN = seq_along))\n\n    # Append the count to the names if count &gt; 1\n    name_counts &lt;- name_counts %&gt;%\n        mutate(unique_names = ifelse(count == 1, names, str_c(names, \" #\", count)))\n\n    return(name_counts$unique_names)\n}\n\ndata$first.name = make_names_unique(data$first.name.raw)\nCode\n# Some people (the Hardy's) put in ridiculous data\n# This is fun for some graphs, but not others\nserious_data = data |&gt;\n    filter(tech.savviness != 10 | first.name == 'Copeland') |&gt;\n    filter(!(first.name.raw %in% c('Bethany', 'Trump', 'Biden', 'Wuss')))\nCode\ntheme_set(theme_xkcd())\ntheme_update(\n    plot.background = element_rect(fill='#222222'),\n    panel.grid.major = element_line(color='#807f7f'),\n    panel.background = element_rect(fill='#3f3f3f'),\n    axis.text = element_text(color='gray90'),\n    plot.title = element_text(color='gray90', size=25),\n    plot.subtitle = element_text(color='gray90', size=20),\n    # plot.subtitle = text,\n    # legend.text = text,\n    # axis.text.x = text,\n    # axis.text.y = text,\n    text=element_text(color='gray90', size=20),\n)\ncolor = '#226796'\nCode\n# I got sick of repeatedly doing this by hand\ngeom_arrow = function(x, y, dx, dy, text='', head='closed', text_dx=0, text_dy=0, arrow_head_len_in=.15){\n    return(list(\n        annotate('segment', xend=x, yend=y, x=x+dx, y=y+dy,\n            arrow=arrow(type = head, length=unit(arrow_head_len_in, 'inches')),\n            color=theme_get()$text$colour,\n        ),\n        annotate('text', x=x+dx+text_dx, y=y+dy+text_dy,\n            label=text,\n            family=theme_get()$text$family,\n            size=theme_get()$text$size / 3,\n            color =theme_get()$text$colour,\n        )\n    ))\n}\nCode\nmean.age = mean((data %&gt;% filter(., !is.na(age)))$age)\nggplot(data, aes(x=age)) +\n    geom_vline(xintercept = 0, color='white') +\n    geom_histogram(bins=25, fill=color, color=theme_get()$panel.background$fill) +\n    # scale_x_binned(transform = 'sqrt')\n    geom_arrow(x=1000, y=2, dx=-40, dy=2, text='2/3 Nephites', text_dy=1) +\n    geom_vline(xintercept = mean.age, color='red') +\n    annotate('text', x=mean.age + 130, y=18, label=paste0('Average age is: ', round(mean.age, 1)), color=theme_get()$text$colour, size=6, family=theme_get()$text$family,) +\n    geom_arrow(x=-73, y=1, dx=80, dy=10, text='One person from the future', text_dy=1) +\n    labs(\n        title='The Distrobution of Ages at the Family Reunion',\n        y='Number of People',\n        x='Age'\n    )\nCode\ndata |&gt; arrange(desc(age)) |&gt; head(8) |&gt; select(name, age) |&gt; clipr::write_clip(allow_non_interactive = T)\nCode\ndata |&gt;\n    filter(!is.na(rating)) |&gt;\nggplot(aes(x=rating)) +\n    geom_histogram(bins=11, color=theme_get()$panel.background$fill, fill=color) +\n    scale_x_binned(breaks = 0:10) +\n    labs(title = \"How much Everyone Enjoys the Family Reunion\")\nCode\nserious_data %&gt;%\n    filter(injuries &gt; 1) |&gt;\nggplot(aes(x=reorder(first.name, desc(injuries)), y=injuries)) +\n    geom_col(fill=color) +\n    geom_arrow(x=3.5, y=8, dx=3, dy=-.5, text='Clearly, I underestimated how much some people get hurt (TYLER)', text_dy=-.2) +\n    theme(axis.text.x = element_text(angle=45,vjust = .7)) +\n    labs(title='People who were Injured the Most in the Last Year', x='')\nCode\nggplot(serious_data |&gt; filter(num.trips &gt; 2), aes(x=reorder(first.name, desc(num.trips)), y=num.trips)) +\n    geom_col(fill=color) +\n    theme(axis.text.x = element_text(angle=45, vjust = .9, hjust = .5)) +\n    scale_y_continuous(breaks=seq(0, 9, 1)) +\n    labs(title = 'Who takes the Most Trips', x='', y='Number of trips')\nCode\n# eval: false\n# echo: false\n# I cannot for the life of me figure out why it's doing this. It just started doing it randomly. I'm starting\n# to seriously suspect a bug in ggplot2.\nhonorable_mentions = c('Rice and butter', 'Pinto beans, Kale, and herbs on sourdough bread toasted with cheese on top.', 'Mash sauce', 'I love food', 'Horse apple', 'Danaes famous Grilled salmon', 'Chaos', 'Chinese takeout', 'Anything grandma makes', 'Black bean, rice, bell pepper, and mango with a lime dressing')\n# honorable_mentions = c()\ndata %&gt;%\n    group_by(fav.recipe) %&gt;%\n    mutate(order = n()) |&gt;\n    filter(!is.na(fav.recipe)) |&gt;\n    filter(order &gt; 1 | fav.recipe %in% honorable_mentions) |&gt;\nggplot(aes(y=reorder(fav.recipe, order))) +\n    geom_bar(orientation = 'y', stat='count', fill=color) +\n    scale_x_continuous(breaks=0:6) +\n    annotate('text', x=3, y=10.7, label='Honorable Mentions') +\n    geom_hline(yintercept = 10.5, color='#cc312c') +\n    labs(title = 'Everyone\\'s Favorite Family Recipies', y='', x='Number of people who say it\\'s their favorite')\nCode\ndata |&gt;\n    group_by(first.name.raw) %&gt;%\n    mutate(order = n()) |&gt;\n    filter(order &gt; 1) |&gt;\n    filter(!is.na(first.name.raw)) |&gt;\nggplot(aes(y=reorder(first.name.raw, order))) +\n    geom_bar(fill=color) +\n    geom_arrow(x=3.5, y=2.6, dx=-.5, dy=-.7, text='Bethany is REALLY coming', text_dy = -.08) +\n    labs(title = 'How Many of Us Share the Same First Name', y='', x='How many times someone with that first name took the survey')\nCode\nstr_split(na.omit(serious_data$ordinances.self), ',') |&gt;\n    flatten() |&gt;\n    tibble() |&gt;\n    rename(Ordinance=1) |&gt;\n    mutate(Ordinance=str_trim(Ordinance)) |&gt;\n    count(Ordinance) |&gt;\n    rename(count=n) |&gt;\n    mutate(count = as.character(count), Ordinance=as.character(Ordinance)) |&gt;\nggplot(aes(x=Ordinance, y=count)) +\n    geom_col(fill=color) +\n    geom_arrow(x=3, y=0, dx=0, dy=.5, \"I totally know how to spell initiatiories\", text_dy=.05) +\n    # scale_y_continuous(breaks = 0:6) +\n    labs(title = 'How many live ordinances have happened this year?')\nCode\nserious_data |&gt;\n    filter(age &lt; 200) |&gt;\n    arrange(age) |&gt;\nggplot(aes(x=age, y=tech.savviness)) +\n    geom_point(color=color, size=3) +\n    geom_smooth(method='lm', se=F, na.rm=T) +\n    annotate('text', x=92, y=4.6, label='Surprisingly, there doesn\\'t seem to be a significant correlation between age and tech savviness', size=5, family=theme_get()$text$family, color=theme_get()$text$colour) +\n    labs(y=\"Tech Savviness\", title='Tech Savviness vs. Age')\nCode\noptions(\n    # No idea why it's adding bars unless I size it just right\n    repr.plot.width=7.56,\n    repr.plot.height=8\n)\ndata |&gt;\n    filter(!is.na(names.indexed)) |&gt;\n    mutate(indexed = ifelse(names.indexed &gt; 0, 'did', \"didn't\")) |&gt;\n    group_by(indexed, label=indexed) |&gt;\n    summarize(indexed = n()) |&gt;\nggplot(aes(y=indexed, x='', fill=label, label=glue('{indexed} People {label}'))) +\n    geom_col() +\n    theme(\n        panel.background = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        # axis.text.y = element_blank(),\n        panel.grid=element_blank(),\n        # axis.text = element_blank(),\n        legend.position = 'none',\n        plot.title = element_text(hjust=.5),\n    ) +\n    geom_text(position=position_stack(vjust=.5), family=theme_get()$text$family, size=7) +\n    coord_polar(theta='y') +\n    labs(x='', y='', title='How many People did Indexing this Year', caption=\"Sorry, I used the wrong input on the survey\")\nCode\noptions(\n    # No idea why it's adding bars unless I size it just right\n    repr.plot.width=7.59,\n    repr.plot.height=8\n)\ndata |&gt;\n    filter(!is.na(oldest)) |&gt;\n    group_by(oldest, label=oldest) |&gt;\n    summarize(oldest = n()) |&gt;\nggplot(aes(y=oldest, x='', fill=label, label=glue('{oldest} said {label}'))) +\n    geom_col() +\n    theme(\n        panel.background = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        # axis.text.y = element_blank(),\n        panel.grid=element_blank(),\n        # axis.text = element_blank(),\n        legend.position = 'none',\n        # plot.title = element_text(hjust=.5, family=theme_get()$text$family, size=15),\n        # plot.subtitle = element_text(hjust=.5, family=theme_get()$text$family, size=15),\n    ) +\n    geom_text_repel(position=position_stack(vjust=.5), family=theme_get()$text$family, size=6) +\n    coord_polar(theta='y') +\n    labs(x='', y='', subtitle='Blair is 83% the oldest sibling', title='Who is the oldest sibling?')"
  },
  {
    "objectID": "familyGraphs.html#interesting-lists",
    "href": "familyGraphs.html#interesting-lists",
    "title": "Thueson Family Graphs",
    "section": "Interesting Lists",
    "text": "Interesting Lists\n\n\nCode\nlists = data |&gt; select('name', 'worst.advice', 'best.advice', 'secret', 'fav.game', 'most.told.story', 'suggested.reunion.location', 'suggested.activity', 'fav.part')\n\n\n\n\nCode\noptions(\n    repr.matrix.max.rows=200,\n    repr.matrix.max.cols=200\n)\nlists\n\n\n\nA tibble: 56 × 9\n\n\nname\nworst.advice\nbest.advice\nsecret\nfav.game\nmost.told.story\nsuggested.reunion.location\nsuggested.activity\nfav.part\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nAmy \nSpank your kid \nWhen angry with spouse go to sleep \nI love car chases in movies \nWater fight \nMy birth story \nFiji \nFishing \nPrograms & laughing with everyone \n\n\nDanae Weber\nSleep when the baby sleeps... Do the dishes when the baby does the dishes. Do the laundry when the baby does the laundry.\nDon't be an idiot. Changed my life.\nI'm a pretty good event planner\nSpin the bottle\nGrandma being tired of her boys fighting, so she tossed them in the yard with boxing gloves and told them to duke it out\nHawaii\nStrong gene games\nStaying up late\n\n\nJaren\nI recommend buying a fleet of transports right now\nPraise in public\nI love writing poetry\nStrip crib\nGrandpa Dee ALMOST catching a jack rabbit with his bare hands\nHolland\nWater skiing\nStaying up late\n\n\nBlair\nNA\nCause that is what we do\nHam radio operator\nNA\nNA\nNA\nPickleball\nRenewing friendships\n\n\nMcKelle\nJust take care of yourself\nThuesons love and serve one another!!\nI’m missing the reunion because I’m in Peru!\nNA\nNA\nCamping somewhere new\nNA\nGames\n\n\nKatelan Shakespear\nIt’s ok to go to bed mad at each other\nDon’t let your kids sleep in your bed.\nI hate olives\nRelays, volleyball, pickleball\nWhen she was having babies!\nAnywhere, as long as we have it!\nBig group games\nTalking around the camp fire\n\n\nKelli Thueson\nDon’t go to the doctor’s office if you only have one problem. Wait until you have four or five; that’s how you get the most bang for your buck.'\nMy feelings don’t have an IQ\nI can roll my tongue into the shape of a cover\nPickleball\nBaby Blair throwing up root beer float (i think) on her busty grandma in the car and it going down said grandmother’s blouse.\nHawaii\nSteak on rocks\nGames at night\n\n\nJenelle Thueson\nNA\nKeep smiling\nHow many missions have I been on so far?\nNA\nTeton Dam flood\nPlace doesn’t matter to me\nNA\nBeing together sharing testimonies\n\n\nLyndee\nNA\nNA\nHow many fake teeth I have\nSigns\nGrandpa catching the rabbit\nNA\nEat scones\nRafting and sunshine\n\n\nLauren\nDon't be helpful\nBe brave\nI have no secrets\nShaving cream on pop\nNA\nNA\nNA\nSwimming, okay with cousins\n\n\nMatt\nTry it you might like it.\nJust because it’s fun doesn’t mean it’s not dumb\nI don’t like hand and foot\nPickleball\nLiving in mud lake\nOn Cruise ship\nFamily cruise\nRelaxing / pickleball\n\n\nLydia\nbe yourself\nyou can’t put a price on feeling ✨bonita✨\ni’m the funniest person ever i just don’t show it so i don’t make everyone’s brain explode\nnight games with cousins\ngreat grandma Annie in the pot\nBEACH (Ryan Gosling 2024💖🇺🇸)\nfloat a different river\nfloating the river\n\n\nSteve Thueson\n“Don’t be dumb, even if it’s fun!” Grandma Thueson. JK\n‘Cause that’s what we do” Grandpa Dee\nWhen I was eight me and my friend went skinny-dipping in the canal around the Rexburg golf course!\nWater fight\nRelated to William Bradford\nPhilippines\nJust be there\nCatching up, seeing young cousins play, testimony meeting, karaoke\n\n\nDeidra\nYou like seafood, try the calamari 🤮\nEnjoy every part of raising little kids🤗 even the exhausting parts, they’ll be big before you know it!\nI never had a date try for a goodnight kiss without being asked first 🙅🏼‍♀️😘\n💦Water games are always fun!\n😍 How Grandpa Dee is the most amazingly handsome guy there is! Nobody’s got muscles and those gorgeous blue eyes like grandpa 🥰\nSomewhere 😎 sunny, by hiking and swimming, call about the 🇵🇭Philippines?\nHugs all around 🥰\n😁Visiting, catching up with everyone.\n\n\nDonna Burrows Thueson\nIf somebody gave me bad advice, I put it out of my mind forever when I discovered its value\nTreat your husband like a king and he will treat you like a queen\nI was a champion Jack’s player in the fourth grade\nHand and Foot\nNA\nFrance for the Olympics\nTalk some genealogy\nAll of it\n\n\nTacy Kirkham\nEat that\nConnection is not too much and not too little.\nI have a book I would like to publish one day\n9 square\nGrandpa and her kissin\nDA Ranch\nGestures\nThe peeps!!\n\n\nSusan\nDon’t go to bed angry\nBelieve in God\nI feel guilt about how much I love clay\nPickleball\nWilliam Bradford\nStanley\nPainting, carving\nTalking around a fire\n\n\nSup\nDon’t go to bed angry\nPick your battles\nOwned 32 cars in 25 years of marriage\nPickleball\nLiving in a pig sty when first married\nOregon Coast\n4 wheeling\nFloating the river\n\n\nCopeland Carter\nDo unto others as you would have others do unto you (not a joke, terrible advice)\nIf you're standing on train tracks, and there's a train coming, you don't dig in your heals, you get off the tracks.\nI love rollerblading\nPoo Head\nThe time she and grandpa were in the cemetery and grandpa ran off and tried to lose her\nThe moon\nSkeet Shooting\nFree food\n\n\nMonty\nDon’t let the son go down on your wrath in a marriage\nYour most valuable asset is your own capacity to earn. Or fight naked.\nI sneak raisins\n9 square\nSomething about Governor Bradford\nMount Vernon\nPatriotic moments\nCatching up conversations\n\n\nMadilyn\nDon't join the moshpit\nDon't fall down in a moshpit\nLike to be in the center of the moshpit\nMoshpit\nDave's conception\nThe club\nMoshpit\nThe moshpit\n\n\nBiden\nWork at the homestead\nCoconut oil\nMy name\nKube\nDave’s conception\nAnywhere but sams\nMock congress\nCopeland\n\n\nTrump\nWork at the homestead\nCoconut oil\nWho I am\nAvoiding the aunts\nDave’s conception\nAnywhere but Sam’s\nBlair and Amy debat\nWatching Blair and Amy\n\n\nWuss boy\nNA\nNA\nNA\nNA\nDave’s conception\nOnly Sam’s\nScuba dive\nCopland\n\n\nNoah\nTo ignore my dreams\nProbably shouldn’t kiss me cousins\nI think there are hot cousins\nSpin bottle\nDave’s conception\nMy house\nSpin the bottle\nThe hot cousins\n\n\nBrigham\nDon’t marry Kali\nDon’t be a dancer\nI had a crush on Hannah\nFree babysitting\nDave’s conception\nThe temple\nPee into a toilet at the same time\nThe end\n\n\nAmmon\nCome to earth\nTalk to Blake about insurance\nI’m gay\nWon’t be there\nDave’s conception\nAmy’s backyard\nWho’s the best kisser\nStarring at noah\n\n\nKali\nTry everything\nLove everyone\nHow wise I am\nArt\nDave’s conception\nThe sawtooths\nCard games\nTalking\n\n\nBethany\nDon’t swear\nAlways ALWAYS keep your garments on\nIt’s a wig\nShaving cream in my hair\nDave’s conception\nOn the beach\nEnd early\nLeaving\n\n\nBethany\nChoose female\nDon’t eat the yellow snow\nI’m bald\nBack rubs\nDave’s conception\nSawtooths\nSeeing all your beautiful faces\nEverything\n\n\nBethany\nListen to others.\nI have only given good advice. Never taken it.\nI am filled with hate\nFighting\nDave’s conception\nOn the river\nKayaking\nFighting\n\n\nDave\nDo unto other as was done unto toy\nCorrect way to conceive\nDefinitely not my conception\nTalking to my family\nMy conception\nHere on the mission\nPork and beans\nBlair and Amy arguing\n\n\nJordan Hardy\nDon’t live by Blake\nBe like Blake\nI love Blake\nAnything with Blake\nDave’s conception\nBlake’s house\nTalk to blake\nBlake\n\n\nAutum Hardy\nDon’t vote Biden\nVote biden\nI’m a liberal\nPoo Head\nDave’s conception\nThe abortion clinic\nChew on one piece of jerky all day\nHearing about Dave’s conception\n\n\nJordan Hardy\nNA\nNA\nNA\nHand and Foot\nNA\nBoise\nA group run/walk\nThe Family\n\n\nSavannah Hardy\nTwins are easy\nBe yourself and the people who love that personality will be never force you to change for them\nBoth my thumbs are double jointed\nHand and Foot\nWhen she forced her husband to get up one night to help when everyone was vomiting and other than that he never helped with the kids\nThe Boise area ranch\nSkits with mixed family teams\nThe family games\n\n\nBethany\nGet a dog\nGet a dog\nI really really want to do some arctic kayaking. Probably in Svalbard.\nSpoons\nMaking her sons beat each other up\nSome awesome national park like Glacier or Zions or Yosemite\nSomething water based\nCatching up\n\n\nCarlene\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVisiting\n\n\nShane Ririe\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAddi Hardy\nAlex tells me not to list w to good people.\nMom lets me play video games with her.\nI’m a princess. When I grow up I will live in a castle.\nCandyland\nShe’s never told me a story\nThe beach\nSnow cones\nThe little sand excavator toy\n\n\nKali\nThat McDonald’s has good fries… 🥴\nDon’t go to bed mad at your spouse.\nI was offered a college scholarship to play soccer way back when!\nNA\nKim sitting cross legged on the floor days after the twin’s birth.\nAnywhere in the Mountains\nEvening bonfires\nCatching up with everyone\n\n\nAlex Hardy\nThat Addi and I aren’t twins, and we shouldn’t be with each other, and we should be a part, and we don’t eat with each other\nNA\nThey don’t know how old I am, or my name, and they don’t know what I have in my house, or where my house is.\nFreeze tag\nNA\nNA\nSand\nThe sand and the water and the food and paying\n\n\nMia Hardy\nIdk\nGo on a mission\nIdk\nI don’t really play games\nHow grandpa proposed to grandma\nWhere I live\nVolleyball\nThe food\n\n\nBraden Pittman\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nQelvin Kirkham\nEverything happens for a reason/No use crying over spilled milk.\nSay, Go, Be, Do.\nI have two living grandparents, neither related to me by blood.\nAll of them\nBoxing Gloves\nFlorida\nAirsoft/Nerf battle\nGames\n\n\nShan\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nUnstructured time for chatting\n\n\nCam\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nUnstructured time for chatting\n\n\nRudi\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nUnstructured time for chatting\n\n\nNori\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nUnstructured time for chatting\n\n\nLacey\nFollow the food pyramid\nGo to the temple every week.\nNA\nHand and Foot\nNA\nDoesn’t matter where to me, as long as I get to see everyone!\nPaint\nVisiting\n\n\nTyler Thueson\nIdk\nIf your gonna be dumb you better be tough\nI’m a open book\nWater fight\nBeing poor\nCamping true camping like palisades big elk creek or moddy Medows or felt or Kilgore\nIndian rug races\nThe uncles\n\n\nDax Thueson\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAva Thueson\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nTate Thueson\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMikaela Thueson\nJust do it\nNA\nI’m terrified of hights\nNA\nHow naughty Tyler was as a little boy ;)\nCamping in the mountains\nFun minute to win it games\nTestimony meeting\n\n\nMoriah Carter\nIt's too late to learn something new\nNobody gives me advice: I do my own research\nI completed high school and college in six years total\nRummikub\nDee not liking onion or celery at the beginning of their marriage. One day he saw her getting onion and celery ready for fricassee and he said don't put it in so she didn't and he said it didn't taste right and she shouldn't make it like that ever again\nDurango, Colorado\nSwitching out my fair style lemonade for hard lemonade\nThe unholy amount of fair style lemonade I will consume"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Copeland Carter’s Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEZRegex\n\n\n\nprojects\n\nopen source\n\npython\n\npublished\n\nhighlighted\n\n\n\n\n\n\n\n\n\nOct 17, 2025\n\n\nCopeland Carter\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic State Machine\n\n\n\nprojects\n\nopen source\n\npython\n\npublished\n\nhighlighted\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nCopeland Carter\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Tensorflow\n\n\n\npython\n\nML\n\n\n\n\n\n\n\n\n\nMay 21, 2024\n\n\nCopeland Carter\n\n\n\n\n\n\n\n\n\n\n\n\nGeoDoodle\n\n\n\nprojects\n\nopen source\n\njavascript\n\nhighlighted\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nCopeland Carter\n\n\n\n\n\n\n\n\n\n\n\n\nSquare Packing\n\n\n\nprojects\n\npython\n\nML\n\nhighlighted\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nCopeland Carter\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Project\n\n\n\nprojects\n\npython\n\nML\n\nhighlighted\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nCopeland Carter\n\n\n\n\n\n\n\n\n\n\n\n\nRose (The Relationship Evaluation Algorithm)\n\n\n\npython\n\nopen source\n\nprojects\n\nhighlighted\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nCopeland Carter\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/EZRegex/index.html",
    "href": "posts/EZRegex/index.html",
    "title": "EZRegex",
    "section": "",
    "text": "Once upon a time, I had a project that required a bunch of regular expressions. I kept getting annoyed with the archaic syntax. It’s annoying to work with, and it’s never gonna change, since it’s built into everything nowadays. It’s unreadable, hard to learn, the backslashes mess up string escaping, and you always end up forgetting to escape a space.\n\n\n\nThe solution: write my own Python library, of course!\nI wrote a quick library that overloaded operators to let you do things like word() + number() + literal(\"stuff\") and it would “compile” that expression into \\w+\\d+stuff.\nAs I learned more I of course rewrote it a bunch of times, and now you can do things like word + number + anyof('a', 'b', 'c') + 'stuff'. There’s also a bunch of utilities for debugging, like a test() method, an invert() function (whcih I’ll get to), and it even supports multiple Regex dialects, since Python regex is slightly different than R, which is slightly different than Perl, etc. Granted, only a few dialects are implemented so far, but the framework is there.\n\n\n\nIt’s around this point (3 or 4 full-project rewrites in) that I decided maybe I should do some research and find out if this project already exists.\n“Shouldn’t you do that at the start”, you ask? Yes. Yes you should. Because not only did a similar project exist, and not only did several similar projects exist, but an entire Github repo existed which is just a collection an explanation of similar projects.\nThis was disappointing (I thought I had that idea), but it also let me take inspiration from other projects and improve my own (for instance, instead of only using the + operator, do things like word.group() + ...).\n\n\n\nSo what makes EZRegex different than the dozens of others?\nWell, none of the others (that I could find) supported other Regex dialects other than Python. They also didn’t support inverting regular expressions…\n\n\n\nEver have a regular expression, and you look at it and go “what the crap is that supposed to match?”? Well EZRegex is here to help! I wrote a funciton where you give it a regular expression, and it returns a string guarenteed to match the given regex.\nFinally learning my lesson from before, I did find a couple libraries out there (xeger, sre-yield) which did similar things, but both fell short of what I wanted: * xeger (regex backwards) is obscure, and tends to return unprintable characters, which is suboptimal for debugging * sre-yield is decent, but is more focused on efficiency, and covering the whole space of things the expression could match\nI specifically wanted strings that were easy to debug: for instance, if you pass “+” (a sequence of word characters, for the uninitiated) to it, I don’t want it to return afdslkjlcv, I want it to return an actual word. There were also a number of other improvements, such as limiting the number of characters given for * and +.\nLuckily, I didn’t have to parse the Regex myself: you can just import the Python regex parser from the re module. From there, I just had to traverse the Abstract Syntax Tree (AST) and substitute my own debugging replacements. It took a lot of code, but ended up being fairly straightforward.\n\n\n\nBeing able to do word + number + '-' + group(digit) is pretty nice, but what’s even better than that? A GUI! It started off using streamlit (ah, we love streamlit), which worked pretty well, but I eventually ended up spending more time trying to hack it to do my bidding than actually using it. It also had several small, unfixable bugs, due to how it worked.\n\n\n\nIn case you haven’t heard of pyodide, someone had the bright idea of compiling the Python interpreter to WebAssembly, so you can run Python code client-side in the browser. While this isn’t actually as useful as you would think, it’s absolutely perfect for this project. I was able to write a custom React frontend, host it on Github pages, and have it make calls to my Python library without the added overhead of a custom backend, or having to wait for network calls (enabling me to do things like update the output in real time as you type).\n\n\n\nThere’s plenty of other websites that can help you write regular expressions. I would know, I took inspiration from a bunch of them. So what makes mine special? Well for one, it directly connects to a python library, so you can copy/paste code back and forth easily, and guarentee that it works the same. But mostly it’s:\n\n\n\nYou every play with Scratch? It’s the thing that taught a lot of my generation about programming. Turns out, the JS library it’s built on is called Blockly, and it’s published (open source, I believe) by Google. It’s wonderful, I was able to embed Regex bubble code blocks directly into the page. Which, if you think about it, Regex is optimal canidate for bubble coding. Blocks that you can easily visualize and snap together, some of which can take parameters & scope? It’s perfect! Here’s a snapshot:\n\n\n\nblockly\n\n\nYou have to admit, that’s easier to understand than \\w+(?P&lt;things&gt;\\s+[^\\-]stuff). If you want to check it out, it’s live at ezregex.com.\n\n\n\nIn the end, I’m a little bit saddened, simply because while this project is great, it’s made a little bit more niche by modern LLMs. If you need a large, deterministic regular expression that you can debug, EZRegex is perfect. But for most small, simple patterns, you can probably just ask ChatGPT and it’ll generate you a working expression. Still, it’s my go to any time I need to think through an expression.\n\n\n\n\nEZRegex\nGithub\nPyPI"
  },
  {
    "objectID": "posts/EZRegex/index.html#origins",
    "href": "posts/EZRegex/index.html#origins",
    "title": "EZRegex",
    "section": "",
    "text": "Once upon a time, I had a project that required a bunch of regular expressions. I kept getting annoyed with the archaic syntax. It’s annoying to work with, and it’s never gonna change, since it’s built into everything nowadays. It’s unreadable, hard to learn, the backslashes mess up string escaping, and you always end up forgetting to escape a space."
  },
  {
    "objectID": "posts/EZRegex/index.html#the-solution",
    "href": "posts/EZRegex/index.html#the-solution",
    "title": "EZRegex",
    "section": "",
    "text": "The solution: write my own Python library, of course!\nI wrote a quick library that overloaded operators to let you do things like word() + number() + literal(\"stuff\") and it would “compile” that expression into \\w+\\d+stuff.\nAs I learned more I of course rewrote it a bunch of times, and now you can do things like word + number + anyof('a', 'b', 'c') + 'stuff'. There’s also a bunch of utilities for debugging, like a test() method, an invert() function (whcih I’ll get to), and it even supports multiple Regex dialects, since Python regex is slightly different than R, which is slightly different than Perl, etc. Granted, only a few dialects are implemented so far, but the framework is there."
  },
  {
    "objectID": "posts/EZRegex/index.html#do-your-research-first-not-last",
    "href": "posts/EZRegex/index.html#do-your-research-first-not-last",
    "title": "EZRegex",
    "section": "",
    "text": "It’s around this point (3 or 4 full-project rewrites in) that I decided maybe I should do some research and find out if this project already exists.\n“Shouldn’t you do that at the start”, you ask? Yes. Yes you should. Because not only did a similar project exist, and not only did several similar projects exist, but an entire Github repo existed which is just a collection an explanation of similar projects.\nThis was disappointing (I thought I had that idea), but it also let me take inspiration from other projects and improve my own (for instance, instead of only using the + operator, do things like word.group() + ...)."
  },
  {
    "objectID": "posts/EZRegex/index.html#why-am-i-special",
    "href": "posts/EZRegex/index.html#why-am-i-special",
    "title": "EZRegex",
    "section": "",
    "text": "So what makes EZRegex different than the dozens of others?\nWell, none of the others (that I could find) supported other Regex dialects other than Python. They also didn’t support inverting regular expressions…"
  },
  {
    "objectID": "posts/EZRegex/index.html#inverting",
    "href": "posts/EZRegex/index.html#inverting",
    "title": "EZRegex",
    "section": "",
    "text": "Ever have a regular expression, and you look at it and go “what the crap is that supposed to match?”? Well EZRegex is here to help! I wrote a funciton where you give it a regular expression, and it returns a string guarenteed to match the given regex.\nFinally learning my lesson from before, I did find a couple libraries out there (xeger, sre-yield) which did similar things, but both fell short of what I wanted: * xeger (regex backwards) is obscure, and tends to return unprintable characters, which is suboptimal for debugging * sre-yield is decent, but is more focused on efficiency, and covering the whole space of things the expression could match\nI specifically wanted strings that were easy to debug: for instance, if you pass “+” (a sequence of word characters, for the uninitiated) to it, I don’t want it to return afdslkjlcv, I want it to return an actual word. There were also a number of other improvements, such as limiting the number of characters given for * and +.\nLuckily, I didn’t have to parse the Regex myself: you can just import the Python regex parser from the re module. From there, I just had to traverse the Abstract Syntax Tree (AST) and substitute my own debugging replacements. It took a lot of code, but ended up being fairly straightforward."
  },
  {
    "objectID": "posts/EZRegex/index.html#the-frontend",
    "href": "posts/EZRegex/index.html#the-frontend",
    "title": "EZRegex",
    "section": "",
    "text": "Being able to do word + number + '-' + group(digit) is pretty nice, but what’s even better than that? A GUI! It started off using streamlit (ah, we love streamlit), which worked pretty well, but I eventually ended up spending more time trying to hack it to do my bidding than actually using it. It also had several small, unfixable bugs, due to how it worked."
  },
  {
    "objectID": "posts/EZRegex/index.html#enter-pyodide",
    "href": "posts/EZRegex/index.html#enter-pyodide",
    "title": "EZRegex",
    "section": "",
    "text": "In case you haven’t heard of pyodide, someone had the bright idea of compiling the Python interpreter to WebAssembly, so you can run Python code client-side in the browser. While this isn’t actually as useful as you would think, it’s absolutely perfect for this project. I was able to write a custom React frontend, host it on Github pages, and have it make calls to my Python library without the added overhead of a custom backend, or having to wait for network calls (enabling me to do things like update the output in real time as you type)."
  },
  {
    "objectID": "posts/EZRegex/index.html#again-why-am-i-special",
    "href": "posts/EZRegex/index.html#again-why-am-i-special",
    "title": "EZRegex",
    "section": "",
    "text": "There’s plenty of other websites that can help you write regular expressions. I would know, I took inspiration from a bunch of them. So what makes mine special? Well for one, it directly connects to a python library, so you can copy/paste code back and forth easily, and guarentee that it works the same. But mostly it’s:"
  },
  {
    "objectID": "posts/EZRegex/index.html#blockly",
    "href": "posts/EZRegex/index.html#blockly",
    "title": "EZRegex",
    "section": "",
    "text": "You every play with Scratch? It’s the thing that taught a lot of my generation about programming. Turns out, the JS library it’s built on is called Blockly, and it’s published (open source, I believe) by Google. It’s wonderful, I was able to embed Regex bubble code blocks directly into the page. Which, if you think about it, Regex is optimal canidate for bubble coding. Blocks that you can easily visualize and snap together, some of which can take parameters & scope? It’s perfect! Here’s a snapshot:\n\n\n\nblockly\n\n\nYou have to admit, that’s easier to understand than \\w+(?P&lt;things&gt;\\s+[^\\-]stuff). If you want to check it out, it’s live at ezregex.com."
  },
  {
    "objectID": "posts/EZRegex/index.html#conclusion",
    "href": "posts/EZRegex/index.html#conclusion",
    "title": "EZRegex",
    "section": "",
    "text": "In the end, I’m a little bit saddened, simply because while this project is great, it’s made a little bit more niche by modern LLMs. If you need a large, deterministic regular expression that you can debug, EZRegex is perfect. But for most small, simple patterns, you can probably just ask ChatGPT and it’ll generate you a working expression. Still, it’s my go to any time I need to think through an expression."
  },
  {
    "objectID": "posts/EZRegex/index.html#links",
    "href": "posts/EZRegex/index.html#links",
    "title": "EZRegex",
    "section": "",
    "text": "EZRegex\nGithub\nPyPI"
  },
  {
    "objectID": "posts/Rose/index.html",
    "href": "posts/Rose/index.html",
    "title": "Rose (The Relationship Evaluation Algorithm)",
    "section": "",
    "text": "Being single is hard. At least, it was for me. Some people are jerks. Some people are your future relationships. Hopefully, one person is your future spouse. How can you tell the difference?\nFor me, as with many people, this was a hard problem. It doesn’t help that as soon as you find someone cute, my mind kinda went blank. I’ve seen it in myself, and I’ve seen in my friends. They have all these standards, and then change them as soon as they meet someone, and then getting hurt.\nWell, after one particularly bad breakup, I had had enough. Surely there was a way to figure out if someone wasn’t right for you before dating them and getting hurt, instead of after.\nAnd that was when Rose was born."
  },
  {
    "objectID": "posts/Rose/index.html#the-concept",
    "href": "posts/Rose/index.html#the-concept",
    "title": "Rose (The Relationship Evaluation Algorithm)",
    "section": "The Concept",
    "text": "The Concept\n\nThe Algorithm\nThe idea is you have a list of attributes that are relevant to relationships. Everything you can possibly think of, from deep stuff like, “You feel like you can trust them”, to [hopefully] obvious stuff like, “You like them”, to frivilous stuff like, “They’re tall”. All the attributes have to be phrased in that way.\nThen, you go through all the questions twice. Once, when you’re unbiased, for instance when you’re single, and for each question, ask yourself, “how important is it to me that… [attribute]?”. For example, “How important is it to me that I feel like I can trust them?”. And you answer that on a scale of -1 to 1, -1 being “Absolutely not, I don’t want that”, and 1 being “absolutely, I need that”. You can also set attributes as dealbreakers, but be very sparing about that. A lot of things that you think are dealbreakers may not be. These are your preferences.\nThen you set thresholds for each kind of relationship that reflect how “picky” you are. 10% being “I’ll enter a relationship with them as long as they breathe”, and 90% being “they have to be almost perfect”. You set thresholds for being friends, dating, and marriage. And maybe engagement, depending on how you think of it.\nThen, later on, when you’re thinking about entering, exiting, or changing a relationship with someone, you can think of that person, and for each question, ask yourself “Do I feel like… [attribute]?”. For example, “Do I feel like I can trust them?”. You answer that question on a scale of 0 to 1, 0 being “no, not at all”, and 1 being “yes, entirely”. These are your evaluations\nThen, for each question, you multiply your preferences * your evaluations, and take the weighted average of all the attributes, and compare it against the thresholds you set. If they scored a 70%, and your threshold for dating is 60%, then it makes sense to date them.\n\n\nAdvantages\nThe advantages of using this algorithm, is that you can always add more attributes. The more attributes you have, the more accurate the answer will become. When creating the boilerplate list of attributes for the 3rd Rose incarnation, I did research. I kept a running list on my phone, and whenever I thought of something, I’d write it down. I observed happy couples I knew, and watched for attributes they did or didn’t do. I asked my friends, my roommates, my parents, I interviewed couples.\nThe other advantage, is that you can take into account emotions with it. I know for me, at least, if somebody asks me if I feel a certain way about something/someone, I’m like, I have no idea. That’s a big question. But if you break it down, and ask “How do you feel about them doing this?”, and ask a bunch of smaller questions, those I can ask with more certainty.\nOf course, it’s not perfect. It’s not the end-all be-all of deciding to date someone. It doesn’t elliminate all hurt; you still have to date and get your heart broken to find out the things you do and don’t prefer. Tuning your preferences is the hardest part about it.\nBut I think it’s very helpful. I can think of several couples I know who are divorced, or who have very unhealthy relationships, because they were wearing the “rose-colored glasses” when they were dating, only to take them off once they were married, only to find out afterwards that they weren’t thinking clearly the entire time. Rose is aimed at helping you take off the rose-colored glasses, and evaluate someone objectively, so you can avoid that situation. At the very least, it can help you think about important things and have important discussions with your significant other before you rush into something. Or it could do the opposite, it could help you realize you’re not overthinking, and it does make sense, even though it might not feel like it."
  },
  {
    "objectID": "posts/Rose/index.html#the-implementation",
    "href": "posts/Rose/index.html#the-implementation",
    "title": "Rose (The Relationship Evaluation Algorithm)",
    "section": "The Implementation",
    "text": "The Implementation\n\nCommand Line\nIt started off as a command line Python program. I had a cool idea, I wanted to actually be able to use it and trust it wouldn’t break, and I didn’t trust my ability to write UI’s at the time. So command line it is.\nHere’s the original script, if you’re interested. It’s pretty short, and wasn’t perfect, but it worked.\n\n\nCode\nimport json\nfrom os.path import dirname, join; DIR  = dirname(__file__)\n\n# Specify these 3\nMAX_NUM = 100\nTOLERANCES = {\n    \"Leave Be\": -MAX_NUM,\n    \"Date\":  0,\n    \"Marry\": MAX_NUM * .9\n}\npreferencesFile = join(DIR, \"girlfriendPreferences.json\")\n\nwith open(preferencesFile, 'r') as f:\n    traits = json.load(f)\n\ndef getInput(trait):\n    _input = input(trait.capitalize() + ': ').strip().lower()\n    if _input in 'y ya yeah yes si true definitely accurate'.split():\n        return 1\n    elif _input in ('n', 'no', 'not', 'nien', 'false', 'nope', 'not really'):\n        return 0\n    elif _input in ('none', 'na', 'n/a', 'not applicable'):\n        return None\n    elif _input in ('sure', 'kinda', 'i guess', 'kind of'):\n        return 0.7\n    else:\n        try:\n            mod = float(_input)\n            if mod &gt;= -1 and mod &lt;= 1:\n                return mod\n            else:\n                raise TypeError()\n        except:\n            print(\"Invalid input\")\n            return getInput(trait)\n\ndef applyTolerance(amt):\n    for action, tolerance in sorted(TOLERANCES.items()):\n        if amt &gt;= tolerance:\n            return action\n    raise UserWarning(\"You've somehow scored less than is possible.\")\n\ngirl = net = count = 0\nfor trait, weight in traits.items():\n    modifier = getInput(trait)\n    if modifier is None:\n        continue\n    else:\n        girl  += modifier * weight\n        net   += weight\n        count += 1\n\nif not count:\n    print('I think you need to learn more about this girl.')\n\nprint(f'\\nOut of the {count} traits evaluated, this girl scored {girl / count} out of {net / count}. She is good to {applyTolerance(girl / count).lower()}.')\n\n\nAnd here’s an updated script I wrote a couple months later, after my best friend was trying to deciding whether to get engaged to her then boyfriend. After going through it with her, I actually helped her convince herself that she wasn’t overthinking things (spoiler alert: they’re now married): Rose 2.0\n\n\nPyQt5\nLater on, after talking about it to my dad & cousin, they thought it was a fantastic idea, and encouraged me to work on it some more. I had my parents take it, which was fun. I honestly don’t remember the outcome, but I remember my mom complaining that a “-10 to 10 scale is too hard to understand”. At that point, I had worked on a couple other projects using Qt, so I wrote a user interface that would let people actually use my program, instead of needing to know how to use the command line and such.\nWhile a good idea, and the program is actually relatively useable out of the box, if you know how to clone a repo and run a python program, it’s still not “deployable”. Python really isn’t great for making programs that other people can use.\nThe code for that is in this repo: Rose 3.0\nI actually did end up using it while dating an equally-nerdy girl, who actually suggested we use it on each other for a date. While not really how the program is intended to be used, you’re supposed to fill out your preferences before you meet someone, so you can be objective, it was still a lot of fun, and broke the ice on a lot of deep questions.\n\n\nSpreadsheet\nLater on, I learned linear algebra, statistics, and spreadsheets. I’m not a huge fan of spreadsheets, I feel like generally programming is easier, and scales much better, but they are super useful for small stuff like this. I put all the questions in a spreadsheet, and copied some cells from the spreadsheet we used in my statistics class, and recreated it in Google Sheets (Excel costs money, and I hate Microsoft). I then realized, the way I was calculating the answer, could be improved. If you think of your preferences as a hypothetical person, and think of your optimal hypothetical person and the actual person you’re evaluating as mathmatical vectors, in an N-dimentional space of all people, the problem becomes: is this person you're evaluating within a certain distance away from your optimal person, in this space?. Which can easily be calculated using linear algebra.\nExcept then, I also realized you can ask the question using statistics, and that way you can get a specific P-value, and your “pickiness” value just becomes the threshold of the P-value.\nBut then you could also take into account your uncertainties for each answer you provide, and incorperate that to get a specific uncertainty value of the average…\nI then went down a rabbit hole of linear algebra, statistics, and spreadsheets for about an hour and a half, utill I finally gave up and just asked the girl sitting next to me the entire time if she would like to start dating. She said yes, and a few months later, she said yes again. We’re now married.\n\n\nThe Future\nEventually, I want to rewrite it again as a website, now that I know React. That way it’s actually accessible to other people. It’s been somewhat less relevant, now that I’m married, but I still want to finish it eventually.\nIt also can be generalized to make all sorts of decisions. I used a similar concept when my wife was trying to decide whether to go to vet school or not. It’s sort of like a mathmatically accurate pro-con list."
  },
  {
    "objectID": "posts/SeniorProject/index.html",
    "href": "posts/SeniorProject/index.html",
    "title": "Senior Project",
    "section": "",
    "text": "My senior project was attempting to use a reinforcement learning algorithm based on a Deep Deterministic Policy Gradient model for use with a continuous observation and action space, in order to solve the Square Packing in a Square problem for N=11 squares.\nYou can read the initial proposal here\nThis file/post is intended to be entirely self-contained. You should be able to run/copy it directly and have it all work.\n\n\nSpoiler Alert\n# I did not succeed\n\n\nI created a custom RL enviorment, built off the Gymnasium library, specifically my own personal SimpleGym class, published my own Cope package.\nBoth the action and observation spaces have the shape (3*N, ) (N is the number of squares, so 11) where each square has an x, y and rotation values. The actor’s actions simply get added to the current square positions and rotations (since the actor actions can be negative), so the actor can make small adjustments to squeze the squares closer to each other.\nI went though several different versions of the reward function. Here’s my final version, with explanations of what each part is doing:\n\n\n\nCode\n\nReward Function\n\ndef _get_reward(self):\n    # The score starts off as positive, because we generally prefer longer episodes\n    score = 100 # Linear\n\n    # These are all customizable coefficients\n    small_side_len = 5.5 # Scalar\n    longevity_importance = .5 # Multiplied\n    side_importance = 100 # Multiplied\n    centered_importance = 0 # Exponential\n    boundary_badness = 0 # Linear\n    if not self.boundary:\n        boundary_badness = 0\n\n    # We like longer episodes\n    score += self.steps * longevity_importance\n\n    # This is the \"main\" value we want to optimize: the side length\n    # All the other stuff is to help it optimize this quicker\n    # It's set so \"small\" side lengths set a positive reward,\n    # and \"large\" side lengths set a negative reward\n    score -= (self.side_len - small_side_len) * side_importance\n    # I tried making it exponential for a time (it didn't seem to help)\n    # score -= math.e**(self.side_len * side_importance)\n\n    # We like it if they're in a small area\n    if self.side_len &lt; small_side_len and self.start_config != 'array':\n        score += 200\n\n    # We want to incentivize not touching, instead of disincentivizing touching,\n    # because this way it doesn't also disincentivize longer runs\n    # (if the reward is positive by default (not touching), then a longer run is okay)\n\n    # We don't like it when they overlap at all\n    # This isn't relevant when we don't allow overlap in the first place\n    if self.overlap_area &gt; 0:\n        score -= 100_000\n        # We really don't like it when they overlap -- the configuration is invalid if they do\n        score -= math.e**(self.overlap_area)\n\n    # I don't want them to just push up against the edges\n    # This creates a \"boundary\" on the sides which, when squares are in it, the reward goes down\n    if boundary_badness or centered_importance:\n        for x, y, _rot in self.squares:\n            # Left\n            if x &lt; self.boundary:\n                score -= boundary_badness\n            # Top\n            if y &lt; self.boundary:\n                score -= boundary_badness\n            # Right\n            if abs(x - self.search_space) &lt; self.boundary:\n                score -= boundary_badness\n            # Bottom\n            if abs(y - self.search_space) &lt; self.boundary:\n                score -= boundary_badness\n\n            # We want the squares to be close to the center\n            # This is in the same loop out for the sake of efficiency\n            if centered_importance:\n                score -= (math.e ** dist([x, y], [self.search_space / 2, self.search_space / 2]) * centered_importance) / self.N\n\n    return score"
  },
  {
    "objectID": "posts/SeniorProject/index.html#overview",
    "href": "posts/SeniorProject/index.html#overview",
    "title": "Senior Project",
    "section": "",
    "text": "My senior project was attempting to use a reinforcement learning algorithm based on a Deep Deterministic Policy Gradient model for use with a continuous observation and action space, in order to solve the Square Packing in a Square problem for N=11 squares.\nYou can read the initial proposal here\nThis file/post is intended to be entirely self-contained. You should be able to run/copy it directly and have it all work.\n\n\nSpoiler Alert\n# I did not succeed\n\n\nI created a custom RL enviorment, built off the Gymnasium library, specifically my own personal SimpleGym class, published my own Cope package.\nBoth the action and observation spaces have the shape (3*N, ) (N is the number of squares, so 11) where each square has an x, y and rotation values. The actor’s actions simply get added to the current square positions and rotations (since the actor actions can be negative), so the actor can make small adjustments to squeze the squares closer to each other.\nI went though several different versions of the reward function. Here’s my final version, with explanations of what each part is doing:\n\n\n\nCode\n\nReward Function\n\ndef _get_reward(self):\n    # The score starts off as positive, because we generally prefer longer episodes\n    score = 100 # Linear\n\n    # These are all customizable coefficients\n    small_side_len = 5.5 # Scalar\n    longevity_importance = .5 # Multiplied\n    side_importance = 100 # Multiplied\n    centered_importance = 0 # Exponential\n    boundary_badness = 0 # Linear\n    if not self.boundary:\n        boundary_badness = 0\n\n    # We like longer episodes\n    score += self.steps * longevity_importance\n\n    # This is the \"main\" value we want to optimize: the side length\n    # All the other stuff is to help it optimize this quicker\n    # It's set so \"small\" side lengths set a positive reward,\n    # and \"large\" side lengths set a negative reward\n    score -= (self.side_len - small_side_len) * side_importance\n    # I tried making it exponential for a time (it didn't seem to help)\n    # score -= math.e**(self.side_len * side_importance)\n\n    # We like it if they're in a small area\n    if self.side_len &lt; small_side_len and self.start_config != 'array':\n        score += 200\n\n    # We want to incentivize not touching, instead of disincentivizing touching,\n    # because this way it doesn't also disincentivize longer runs\n    # (if the reward is positive by default (not touching), then a longer run is okay)\n\n    # We don't like it when they overlap at all\n    # This isn't relevant when we don't allow overlap in the first place\n    if self.overlap_area &gt; 0:\n        score -= 100_000\n        # We really don't like it when they overlap -- the configuration is invalid if they do\n        score -= math.e**(self.overlap_area)\n\n    # I don't want them to just push up against the edges\n    # This creates a \"boundary\" on the sides which, when squares are in it, the reward goes down\n    if boundary_badness or centered_importance:\n        for x, y, _rot in self.squares:\n            # Left\n            if x &lt; self.boundary:\n                score -= boundary_badness\n            # Top\n            if y &lt; self.boundary:\n                score -= boundary_badness\n            # Right\n            if abs(x - self.search_space) &lt; self.boundary:\n                score -= boundary_badness\n            # Bottom\n            if abs(y - self.search_space) &lt; self.boundary:\n                score -= boundary_badness\n\n            # We want the squares to be close to the center\n            # This is in the same loop out for the sake of efficiency\n            if centered_importance:\n                score -= (math.e ** dist([x, y], [self.search_space / 2, self.search_space / 2]) * centered_importance) / self.N\n\n    return score"
  },
  {
    "objectID": "posts/SeniorProject/index.html#dependant-code",
    "href": "posts/SeniorProject/index.html#dependant-code",
    "title": "Senior Project",
    "section": "Dependant Code",
    "text": "Dependant Code\nFor reproducibility’s sake, here is the rest of the relevant custom code I used in this project (because although my Cope package is published, it’s not stable):\n\n\n\nRedirectStd\n\nmisc.py\n\nimport io, sys\nfrom random import randint\n\nclass RedirectStd:\n    def __init__(self, stdout=None, stderr=None):\n        if isinstance(stdout, io.TextIOBase):\n            self._stdout = stdout\n        else:\n            self._stdout = open(stdout or os.devnull, 'w')\n        if isinstance(stderr, io.TextIOBase):\n            self._stderr = stderr\n        else:\n            self._stderr = open(stderr or os.devnull, 'w')\n\n    def __enter__(self):\n        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr\n        self.old_stdout.flush(); self.old_stderr.flush()\n        sys.stdout, sys.stderr = self._stdout, self._stderr\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._stdout.flush(); self._stderr.flush()\n        self._stdout.close(); self._stderr.close()\n        sys.stdout = self.old_stdout\n        sys.stderr = self.old_stderr\n\ndef interpret_percentage(percentage:int | float) -&gt; float:\n    if isinstance(percentage, bool):\n        return float(percentage)\n    elif percentage &gt; 1:\n        return percentage / 100\n    return percentage\n\ndef percent(percentage:int | float):\n    ''' Usage:\n        if (percent(50)):\n            &lt;code that has a 50% chance of running&gt;\n        NOTE: .5 works as well as 50\n    '''\n    return randint(1, 100) &lt; interpret_percentage(percentage)*100\n\n\n\n\n\n\nSimpleGym\n\nSimpleGym.py\n\nfrom sys import exit\nfrom abc import ABC\nfrom time import sleep, time as now\nimport gymnasium as gym\nimport pygame\n\nclass SimpleGym(gym.Env, ABC):\n    \"\"\" A simplified Gymnasium enviorment that uses pygame and handles some stuff for you, like rendering\n        keeping track of steps, returning the right things from the right functions, event handling,\n        including some default keyboard shortcuts, and some debugging features. Includes easy ways\n        to print to the screen.\n\n        **Rendering**\n            By default, it's set to render the enviorment according to the function render_pygame, which\n            should render everything to self.surf. If you would like to specify other rendering methods,\n            define them as `render_{method_name}`, and render() will handle it for you. There's no need\n            to manually overwrite the render() method.\n\n        **Printing**\n            There are 3 ways to print to the screen:\n                `show_vars`: accessable either via the constructor or as a member\n                    This is a dictionary of {name: member} of members you want to have printed\n                    on the screen. The keys can be any string, and the values must be valid members\n                    of this class. The screen is updated as the member changes.\n                `show_strings`: accessable either via the constructor, as a member, or the show() function\n                    This is a list of strings that are printed to the screen. They won't change.\n                    Useful for showing config options and the like. They are always printed first.\n                `print`: a dictionary member. The keys are arbitrary and not printed. The values\n                    are printed to the screen. The reason it's a dictionary and not a list is simply\n                    for easy indexing: you can change this in the middle of the running loop, and\n                    it will get added to the screen. Just make sure to reuse the keys.\n                    Attempting to set an item on an instance will also set it to print\n                    (i.e. `env['a'] = 'string'` is the same as `env.print['a'] = 'string'`)\n\n        Default key handling:\n            q closes the window\n            space toggles pause\n            i increments a single frame\n            u runs the debug_button()\n            r runs reset() immediately\n            f toggles whether we're limiting ourselves to FPS or not\n            h shows a help menu on screen\n            &gt;/&lt; increase and decrease the FPS\n\n        In order to use this effectively, you need to overload:\n            __init__(), if you want to add any members\n            _get_obs()\n            _get_reward()\n            _step(action), don't overload step(), step() calls _step\n            _reset(seed=None, options=None), if you need any custom reset code (don't overload reset())\n            render_pygame(), render to self.surf\n        and optionally:\n            _get_terminated(), if you want to use custom terminated criteria other than just max_steps\n                You *don't* need to call super()._get_terminated()\n            _get_info(), if you want to include info\n            handle_event(event), for handling events\n            debug_button(), for debugging when you press the u key\n            _get_truncated(), if you want to include truncated\n\n        Helpful members provided:\n            `width`, `height`: the dimentions of the screen\n            `size`: set to self.width, for compatibility's sake\n            `just_reset`: is set to True by reset() and False by step()\n            `steps`: the number of steps in the current episode\n            `total_steps`: the total number of steps taken\n            `reset_count`: a count of the number of times reset() has been called\n            `surf`: the surface to draw to in render_pygame()\n            `paused`: True if paused\n            `increment`: set to True internally to denote a single step while paused. step() sets\n                to False\n    \"\"\"\n\n    FPS_STEP = 2\n    SHOW_HELP_FADE_TIME = 10\n    FONT_SIZE = 10\n    HELP_TEXT = \"\"\"\n        q: close window\n        space: toggle pause\n        i: increment a single frame\n        u: run debug_button()\n        r: reset\n        f: toggle limit FPS\n        &lt;/&gt;: change FPS\n        d: toggle displaying\n        h: show/hide this menu\n    \"\"\"\n\n    def __init__(self,\n        max_steps=None,\n        screen_size=300,\n        fps=None,\n        name='SimpleGym Enviorment',\n        show_vars={},\n        show_strings=[],\n        start_paused=False,\n        render_mode='pygame',\n        displaying=True,\n        print=True,\n        assert_valid_action=True,\n        background_color=(20, 20, 20),\n        print_color=(200, 20, 20, 0),\n        show_events=False,\n        verbose=True,\n    ):\n        \"\"\" This should be called first, if you  want to use the members like self.size\n            Parameters:\n                `max_steps`: if positive, sets the maximum number of steps before the env resets\n                    itself. If None or negative, no limit\n                `screen_size`: the size of the pygame window. Can be a 2 item tuple of (width, height)\n                    or a single int if the window is to be square\n                `fps`: controls how fast the simulation runs. Set to negative or None to have no limit\n                `name`: the name of the enviorment shown on the window title\n                `show_vars`: a dictionary of {name: member} of members you want to have printed\n                    on the screen. The keys can be any string, and the values must be valid members\n                    of this class\n                `show_strings`: a list of strings you want to have printed on the screen\n                `start_paused`: self-explanitory\n                `show_events`: prints events, other than mouse movements, for debugging purpouses\n                `render_mode`: part of the gymnasium specification. Must be either None or 'pygame',\n                    unless you manually override the render() method\n                `displaying`: turn off to not render, to go faster\n                `print`: whether we automatically render self.print to the screen. You can use self.display() to\n                    display to a Jupyter Notebook instead. It still renders show_strings.\n                `assert_valid_action`: ensures that actions given to step() are within the action_space\n                `background_color`: a 3 item tuple specifying the background color\n                `print_color`: a 4 item tuple (the 4th index being alpha) specifying the color of\n                    the extra data printed to the screen\n                `verbose`: when set to True, it simply adds `fps`, `reset_count`, `steps`, `total_steps`\n                    to `show_vars`. Also shows the help menu for the first few seconds\n        \"\"\"\n        self.metadata = {\"render_modes\": list({'pygame', render_mode}), \"render_fps\": fps}\n        assert render_mode is None or render_mode in self.metadata[\"render_modes\"], render_mode\n\n        self.background_color = background_color\n        self.print_color = print_color\n        self.name = name\n        self.show_events = show_events\n        self.fps = self.FPS = fps\n\n        self.max_steps = max_steps\n        self.steps = 0\n        self.total_steps = 0\n        self.paused = start_paused\n        self.increment = False\n        self.just_reset = False\n        self.reset_count = 0\n\n        self.render_mode = render_mode\n        self.screen_size = screen_size if isinstance(screen_size, (tuple, list)) else (screen_size, screen_size)\n        self.width, self.height = self.screen_size\n        self.size = self.width\n        self.screen = None\n        self.surf = None\n        self.font = None\n        self._displaying = displaying\n        self._print = print\n\n        self.print = {}\n        self.show_strings = show_strings\n        self.show_vars = show_vars\n        if verbose:\n            self.show_vars['FPS'] = 'fps'\n            self.show_vars['Episode'] = 'reset_count'\n            self.show_vars['Step'] = 'steps'\n            self.show_vars['Total Steps'] = 'total_steps'\n\n        # We want to ensure that reset gets called before step. Nowhere else does this get set to False\n        self._previous_step_called = None\n        self._assert_valid_action = assert_valid_action\n        self._prints_surf = None\n        self._original_fps = fps\n        self._show_help = verbose\n        self._rendered_helps = None\n\n    def _get_obs(self):\n        raise NotImplementedError\n\n    def _get_info(self):\n        return {}\n\n    def _get_truncated(self):\n        return False\n\n    def __default_get_terminated(self):\n        \"\"\" Called internally so max_steps still works even if _get_terminated is overloaded \"\"\"\n        term = False\n        if self.max_steps is not None and self.max_steps &gt; 0 and self.steps &gt; self.max_steps:\n            term = True\n\n        return self._get_terminated() or term\n\n    def _get_terminated(self):\n        \"\"\" By default this just terminates after max_steps have been reached \"\"\"\n        return False\n\n    def _get_reward(self):\n        raise NotImplementedError\n\n    def reset(self, seed=None, options=None):\n        \"\"\" This sets the self.np_random to use the seed given, resets the steps, and returns the\n            observation and info. Needs to be called first, if you're depending on self.np_random,\n            or steps equalling 0, but also needs to return what this returns.\n        \"\"\"\n        # We need the following line to seed self.np_random\n        super().reset(seed=seed)\n\n        self._reset(seed=seed, options=options)\n\n        self.steps = 0\n        self.just_reset = True\n        self.reset_count += 1\n\n        return self._get_obs(), self._get_info()\n\n    def _reset(seed=None, options=None):\n        raise NotImplementedError()\n\n    def step(self, action):\n        \"\"\" Call this last, and return it \"\"\"\n        assert self.reset_count &gt; 0, \"step() called before reset(). reset() must be called first.\"\n\n        # If it's paused, don't bother checking if it's a valid action\n        if self._assert_valid_action and not self.paused or self.increment:\n            assert self.action_space.contains(action), \"Action given not within action_space\"\n\n        if self.paused and not self.increment:\n            return self._get_obs(), self._get_reward(), self.__default_get_terminated(), self._get_truncated(), self._get_info()\n\n        self._step(action)\n\n        if not self.paused or self.increment:\n            self.steps += 1\n            self.total_steps += 1\n        self.just_reset = False\n        self.increment = False\n\n        if self.fps is not None and self.fps &gt; 0 and self._previous_step_called is not None:\n            # Sleep for the amount of time until we're supposed to call step next\n            wait_for = (1/self.fps) - (now() - self._previous_step_called)\n            # If calculations elsewhere took so long that we've already past the next frame time,\n            # don't sleep, just run\n            if wait_for &gt; 0:\n                sleep(wait_for)\n\n        self._previous_step_called = now()\n        return self._get_obs(), self._get_reward(), self.__default_get_terminated(), self._get_truncated(), self._get_info()\n\n    def _step(action):\n        raise NotImplementedError()\n\n    # @ensure_imported(pygame)\n    def _init_pygame(self):\n        import pygame\n        if self.screen is None:\n            pygame.init()\n            pygame.display.init()\n            pygame.display.set_caption(self.name)\n            self.screen = pygame.display.set_mode(self.screen_size)\n\n        if self.font is None:\n            self.font = pygame.font.SysFont(\"Verdana\", self.FONT_SIZE)\n\n        if self.surf is None:\n            self.surf = pygame.Surface(self.screen_size)\n            self.surf.convert()\n            # self.surf.fill((255, 255, 255))\n\n        if self._prints_surf is None:\n            self._prints_surf = pygame.Surface(self.screen_size)\n            self._prints_surf.convert()\n            self._prints_surf.fill(self.background_color)\n\n        if self._rendered_helps is None:\n            self._rendered_helps = [\n                self.font.render(line, True, self.print_color)\n                for line in self.HELP_TEXT.splitlines()\n            ]\n\n    def render(self):\n        if not self._displaying:\n            self._handle_events()\n            return\n\n        if self.render_mode == 'pygame':\n            self._init_pygame()\n            # Fill the background\n            self.surf.fill(self.background_color)\n\n            self.render_pygame()\n\n            # The strings in show_strings should come first\n            strings = self.show_strings.copy()\n            # Get the texts from self.show_vars\n            length = len(max(self.show_vars.keys(), key=len))\n            strings += [\n                f'{name}: {\" \"*(length - len(name))} {getattr(self, var, f\"{var} is not a member\")}'\n                for name, var in self.show_vars.items()\n            ]\n\n            if self._print:\n                # Add the texts from self.prints\n                strings += list(self.print.values())\n\n                # Draw all the text onto the surface\n                for offset, string in enumerate(strings):\n                    self.surf.blit(self.font.render(string, True, self.print_color), (5, 5 + offset*(self.FONT_SIZE + 2)))\n\n                if self._show_help:\n                    for offset, string in enumerate(self._rendered_helps):\n                        max_width = max(self._rendered_helps, key=lambda h: h.get_size()[0]).get_size()[0]\n                        self.surf.blit(string, (self.width - max_width, offset*(self.FONT_SIZE + 2)))\n\n            # If we're not displaying, but still running, show some sort of indication, so they know we haven't frozen\n            if not self._displaying:\n                self.surf.blit(pygame.font.SysFont(\"Verdana\", 25).render('PAUSED', True, self.print_color), (100, 50))\n\n            # I don't remember what this does, but I think it's important\n            self.surf = pygame.transform.scale(self.surf, self.screen_size)\n\n            # Display to screen\n            self.screen.blit(self.surf, (0, 0))\n            self._handle_events()\n            pygame.display.flip()\n\n        else:\n            if hasattr(self, f'render_{self.render_mode}'):\n                getattr(self, f'render_{self.render_mode}')()\n            else:\n                raise AttributeError(f\"No render_{self.render_mode} function provided\")\n\n    def debug_button(self):\n        pass\n\n    def _handle_events(self):\n        for e in pygame.event.get():\n            match e.type:\n                case pygame.QUIT:\n                    self.close()\n                    exit(0)\n                case pygame.KEYDOWN:\n                    if e.key == pygame.K_ESCAPE:\n                        self.close()\n                        exit(0)\n                    elif e.key == pygame.K_SPACE:\n                        self.paused = not self.paused\n\n                    match e.unicode:\n                        case 'q':\n                            self.close()\n                            exit(0)\n                        case 'u':\n                            self.debug_button()\n                        case 'i':\n                            self.increment = True\n                            # If it's not paused, make it paused\n                            self.paused = True\n                        case 'r':\n                            self.reset()\n                        case 'f':\n                            if self.fps is None:\n                                self.fps = self._original_fps\n                            else:\n                                self._original_fps = self.fps\n                                self.fps = None\n                        case '&gt;' | '.':\n                            if self.fps is None:\n                                self.fps = self.FPS_STEP\n                            else:\n                                self.fps += self.FPS_STEP\n                        case '&lt;' | ',':\n                            if self.fps is None:\n                                self.fps = 0\n                            else:\n                                self.fps -= self.FPS_STEP\n                        case 'h':\n                            self._show_help = not self._show_help\n                        case 'd':\n                            self._displaying = not self._displaying\n                        case _:\n                            self.handle_event(e)\n                case _:\n                    self.handle_event(e)\n                    if self.show_events and e.type != pygame.MOUSEMOTION:\n                        print(e)\n        pygame.event.pump()\n\n    def handle_event(self, event):\n        pass\n\n    def show(self, string):\n        \"\"\" Sets a given string to be shown on the pygame window \"\"\"\n        self.show_strings.append(string)\n\n    def __setitem__(self, index, string):\n        self.print[index] = string\n\n    def display(self, to=None):\n        from IPython.display import HTML, display\n        if to is not None:\n            to.update(HTML('&lt;br/&gt;'.join(self.show_strings + list(self.print.values()))))\n        else:\n            display(HTML('&lt;br/&gt;'.join(self.show_strings + list(self.print.values()))), clear=True)\n\n    def close(self):\n        if self.screen is not None:\n            pygame.display.quit()\n            pygame.quit()\n            self.screen = None\n            self.font = None\n\n\n\n\n\n\nSquareEnv\n\nSquareEnv2.py\n\nimport itertools\nimport gymnasium as gym\nimport random\nfrom typing import Iterable, Literal\nfrom gymnasium import spaces\nimport shapely.geometry as sg\nimport shapely.ops as so\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pygame\nfrom pygame import gfxdraw\nimport numpy as np\nfrom typing import List, Tuple\nimport math\nfrom math import cos, dist, sin, tan, pi\nfrom shapely.geometry import MultiPolygon, Polygon, Point\nfrom shapely.affinity import rotate\nfrom shapely.ops import unary_union\n# from Cope.gym import SimpleGym\n# This is safe to remove if you're using pygame or nothing to render\nfrom IPython.display import display\n\n\ndef multiPolygon2Space(multi):\n    rtn = []\n    for geom in multi.geoms:\n        # Compute the x, y, and rotation angle values for this square\n        # Skip the last coordinate, because the first and last are the same\n        corner1, corner2, corner3, corner4, _ = geom.exterior.coords\n\n        rot_rad = math.atan2(corner4[1] - corner3[1], corner4[0] - corner3[0])\n        # Normalize the angles to all be positive (due to our space requirements)\n        # if rot_rad &lt; 0:\n        #     rot_rad += math.pi/2\n\n        # Add the x, y, and rotation angle values to the result list\n        rtn.append(np.array((\n            # x\n            (corner1[0] + corner2[0] + corner3[0] + corner4[0]) / 4,\n            # y\n            (corner1[1] + corner2[1] + corner3[1] + corner4[1]) / 4,\n            # Rotation (radians)\n            rot_rad\n        )))\n\n    return np.array(rtn)\n\ndef space2MultiPolygon(space, side_len=1):\n    # Autoreshape it if it's flat\n    if len(space.shape) == 1:\n        space = space.reshape((int(len(space)/3), 3))\n    # return MultiPolygon([Polygon(corners) for corners in compute_corners(space, sideLen=side_len)])\n    return MultiPolygon(map(Polygon, compute_all_corners(space, side_len=side_len)))\n\ndef compute_all_corners(squares: List[Tuple[float, float, float]], side_len=1) -&gt; np.ndarray:\n    # return np.array([compute_corners(square) for square in squares])\n    return np.array(list(map(lambda s: compute_corners(s, side_len=side_len), squares)))\n\ndef compute_corners(square: Tuple[float, float, float], side_len=1) -&gt; np.ndarray: #[float, float, float, float]:\n    # Rotation is in radians\n    x, y, rot = square\n    # Compute the coordinates of the four corners of the square\n    half = side_len / 2\n    return np.array([\n        (x + corner[0]*math.cos(rot) - corner[1]*math.sin(rot),\n         y + corner[0]*math.sin(rot) + corner[1]*math.cos(rot))\n        for corner in [(half, half), (half, -half), (-half, -half), (-half, half)]\n    ])\n\nclass SquareEnv(SimpleGym):\n    def __init__(self, *args,\n                 N            = 4,\n                 search_space = None,\n                 shift_rate   = .01,\n                 rot_rate     = .001,\n                #  flatten      = False,\n                 max_steps    = 1000,\n                 boundary     = 0,\n                 max_overlap  = .5,\n                 start_config:Literal['valid', 'array', 'random'] = 'valid',\n                 screen_size  = None,\n                 # mixed is loop the rotation, but clip the position\n                 bound_method:Literal['clip', 'loop', 'mixed'] = 'mixed',\n                 disallow_overlap=False,\n                 **kwargs,\n            ):\n        \"\"\" N is the number of boxes\n            search_space is the maximum length of the larger square we're allowing the smaller\n                squares to be in\n            shift_rate is the maximum rate at which we can shift the x, y values per step\n            rot_rate is the maximum rate at which we can rotate a box per step\n            start_config defines we we reset the squares.\n                If `random`, the boxes are randomly placed\n                If `valid`, the boxes are randomly placed, but not overlapping\n                If `array`, the boxes are arrayed in a grid, but are randomly \"jiggled\"\n        \"\"\"\n        super().__init__(\n            *args,\n            max_steps=max_steps,\n            screen_size=screen_size,\n            background_color=(255, 255, 255),\n            print_color=(0, 0, 0),\n            name='Square Packing',\n            show_vars={'FPS': 'fps'},\n            assert_valid_action=False,\n            verbose=False,\n            **kwargs\n        )\n        self._show_help = True\n\n        if search_space is None:\n            search_space = N\n\n        # self.render_mode = render_mode\n        self.search_space = search_space\n        self.shift_rate = shift_rate\n        self.rot_rate = rot_rate\n        self.N = N\n        # self.steps = 0\n        self.scale = 20\n        self.offset = 50\n        self.bound_method = bound_method.lower()\n        self.max_overlap = max_overlap\n        self.max_steps = max_steps\n        self.boundary = boundary\n        self.disallow_overlap = disallow_overlap\n        size = self.search_space*self.scale+(self.offset*2)\n        if screen_size is None:\n            self.screen_size = np.array((size, size))\n        # self.screen = None\n        # self.surf = None\n        # self.extraNums = None\n        # self.userSurf = None\n        # self.userSurfOffset = 0\n        # self._userPrinted = False\n        # self.font = None\n        # self._flat = flatten\n        self.start_config = start_config\n        self.movement = np.zeros((self.N, 3))\n        self.squares: np.ndarray # with the shape (N, 3): it gets flattened/reshaped to interface with the spaces\n\n        ### Define the spaces ###\n        # if self._flat:\n        self.observation_space = spaces.Box(low=np.zeros((N*3,)),\n            high=np.array([[search_space]*N, [search_space]*N, [math.pi/2]*N]).T.flatten(),\n            dtype=np.float64, shape=(N*3,))\n\n        # The action space is shifting & rotating the squares little bits at a time\n        self.action_space = spaces.Box(low=np.array([[-shift_rate]*N, [-shift_rate]*N, [-rot_rate]*N]).T.flatten(),\n            high=np.array([[shift_rate]*N, [shift_rate]*N, [rot_rate]*N]).T.flatten(),\n            dtype=np.float64, shape=(N*3,))\n\n        # else:\n        #     self.observation_space = spaces.Box(low=np.zeros((N,3)),\n        #                                         high=np.array([[search_space]*N, [search_space]*N, [math.pi/2]*N]).T,\n        #                                         dtype=np.float64, shape=(N,3))\n\n        #     # The action space is shifting & rotating the squares little bits at a time\n        #     self.action_space = spaces.Box(low=np.array([[-shift_rate]*N, [-shift_rate]*N, [-rot_rate]*N]).T,\n        #                                 high=np.array([[shift_rate]*N, [ shift_rate]*N, [ rot_rate]*N]).T,\n        #                                 dtype=np.float64, shape=(N,3))\n\n    def _get_obs(self):\n        return self.squares.flatten()\n\n    def _get_info(self):\n        return {\n            # 'Overlaps': not self.squares.is_valid,\n            'overlap': self.overlap_area,\n            'len': self.side_len,\n            'wasted': self.wasted_space,\n            # 'loss': lossFunc(self.squares),\n        }\n\n    def _get_terminated(self):\n        # Optimal: 3.789, best known: 3.877084\n        # There's no overlapping and we're better than the previous best\n        if self.N == 11 and self.side_len &lt; 3.877084 and self.is_valid():\n            print('Holy cow, we did it!!!')\n            print('Coordinates & Rotations:')\n            print(self.squares)\n            with open('~/SQUARE_PARAMETERS.txt', 'w') as f:\n                f.write(str(self.squares))\n            return True\n\n        # If we're almost entirely overlapping, just kill it\n        if self.overlap_area &gt; self.max_overlap:\n            return True\n\n        # If we're pretty small, and we're only making small adjustments, don't reset, we're doing good!\n        if not np.any(np.median(self.movement, axis=0)) and self.side_len &gt; 4.5:\n            return True\n\n        return False\n\n    def _get_reward(self):\n        # We generally prefer living longer\n        score = 100 # Linear\n        small_side_len = 5.5 # Scalar\n        longevity_importance = .5 # Multiplied\n        side_importance = 100 # Multiplied\n        centered_importance = 0 # Exponential\n        boundary_badness = 0 # Linear\n        if not self.boundary:\n            boundary_badness = 0\n\n        score += self.steps * longevity_importance\n\n        # score -= math.e**(self.side_len * side_importance)\n        score -= (self.side_len - small_side_len) * side_importance\n\n        # We like it if they're in a small area\n        if self.side_len &lt; small_side_len and self.start_config != 'array':\n            score += 200\n\n        # We want to incentivize not touching, instead of disincentivizing touching,\n        # because this way it doesn't also disincentivize longer runs\n        # (if the reward is positive by default (not touching), then a longer run is okay)\n\n        # We don't like it when they overlap at all\n        if self.overlap_area &gt; 0:\n            score -= 100_000\n            # We really don't like it when they overlap\n            score -= math.e**(self.overlap_area)\n\n        # This is essentially a percentage of how much they're overlapping\n        # score -= self.overlap_area / (self.N - self.max_overlap)**2\n\n        # I don't want them to just push up against the edges\n        if boundary_badness or centered_importance:\n            for x, y, _rot in self.squares:\n                # Left\n                if x &lt; self.boundary:\n                    score -= boundary_badness\n                # Top\n                if y &lt; self.boundary:\n                    score -= boundary_badness\n\n                # Right\n                if abs(x - self.search_space) &lt; self.boundary:\n                    score -= boundary_badness\n                # Bottom\n                if abs(y - self.search_space) &lt; self.boundary:\n                    score -= boundary_badness\n\n                # We want the squares to be close to the center\n                if centered_importance:\n                    score -= (math.e ** dist([x, y], [self.search_space / 2, self.search_space / 2]) * centered_importance) / self.N\n\n        return score\n\n    def _step(self, action):\n        # The action is given flattened, but self.squares looks like [[x, y, radians], ...]\n        assert action.shape == (self.N*3,), f'Action given to step is the wrong shape (Expected shape ({self.N*3},), got {action.shape})'\n        action = action.reshape((self.N,3))\n\n        # Compute the shifted squares\n        new_squares = self.squares + action\n\n        # Make sure we don't leave the observation space\n        if self.bound_method == 'clip':\n            new_squares[:,:2][new_squares[:,:2] &gt;  self.search_space] = self.search_space\n            new_squares[:,:2][new_squares[:,:2] &lt; 0]                  = 0\n            new_squares[:,2][new_squares[:,2]   &gt; math.pi/2]          = math.pi/2\n            new_squares[:,2][new_squares[:,2]   &lt; 0]                  = 0\n        elif self.bound_method == 'loop':\n            new_squares[:,:2][new_squares[:,:2] &gt;  self.search_space] = 0\n            new_squares[:,:2][new_squares[:,:2] &lt; 0]                  = self.search_space\n            new_squares[:,2][new_squares[:,2]   &gt; math.pi/2]          = 0\n            new_squares[:,2][new_squares[:,2]   &lt; 0]                  = math.pi/2\n        # Loop the rotation, but clip the position\n        elif self.bound_method == 'mixed':\n            new_squares[:,:2][new_squares[:,:2] &gt;  self.search_space] = self.search_space\n            new_squares[:,:2][new_squares[:,:2] &lt; 0]                  = 0\n            new_squares[:,2][new_squares[:,2]   &gt; math.pi/2]          = 0\n            new_squares[:,2][new_squares[:,2]   &lt; 0]                  = math.pi/2\n        else:\n            raise TypeError(f'Unknown `{self.bound_method}` bound_method provided')\n\n        if self.disallow_overlap:\n            for i1, square1 in enumerate(new_squares):\n                # i1+1, because if a intercets b, then b intersects a. We don't need to check it again\n                # We also don't need to check if a intersects a.\n                for i2, square2 in enumerate(new_squares[i1+1:], start=i1+1):\n                    if Polygon(compute_corners(square1)).intersects(Polygon(compute_corners(square2))):\n                        new_squares[i1] = self.squares[i1]\n                        new_squares[i2] = self.squares[i2]\n\n        self.movement = self.squares - new_squares\n        self.squares = new_squares\n\n    def _reset(self, seed=None, options=None):\n        # Why does the Space constructor have a seed and not the .sample() method??\n        if seed is None:\n            # We can't be deterministic AND auto start at a valid point\n            # Also make sure we're within the boundaries\n            match self.start_config:\n                case 'random':\n                    self.squares = self.observation_space.sample().reshape((self.N, 3))\n                case 'valid':\n                    self.squares = self.observation_space.sample().reshape((self.N, 3))\n                    while not self.within_boundary or not self.is_valid(False):\n                        self.squares = self.observation_space.sample().reshape((self.N, 3))\n                case 'array':\n                    cols = math.ceil(math.sqrt(self.N))\n                    added = 0\n                    # Minimum so they can't overlap: 1.4142135623730951 (math.sqrt((.5**2)*2) * 2)\n                    gap = 2\n                    startx = self.boundary + gap / 2\n                    starty = self.boundary + gap / 2\n                    squares = []\n                    x = startx\n                    y = starty\n                    col = 0\n                    while added &lt; self.N:\n                        squares.append([x, y, random.uniform(0, math.pi/2)])\n                        added += 1\n                        col += 1\n\n                        if col &gt;= cols:\n                            y += gap\n                            x = startx\n                            col = 0\n                        else:\n                            x += gap\n                    self.squares = np.array(squares)\n                case _:\n                    ValueError('Invalid start_config parameter given')\n\n        else:\n            # This is untested after the refactor\n            # if self._flat:\n            self.squares = spaces.Box(\n                low  =np.zeros((self.N,3)),\n                high =np.array([[self.search_space]*self.N, [self.search_space]*self.N, [math.pi/2]*self.N]).T,\n                dtype=np.float64,\n                shape=(self.N,3),\n                seed =seed,\n            ).sample()\n            # else:\n            #     self.squares = spaces.Box(low=np.zeros((self.N*3,)),\n            #                     high=np.array([[self.search_space]*self.N, [self.search_space]*self.N, [math.pi/2]*self.N]).T.flatten(),\n            #                     dtype=np.float64, shape=(self.N*3,), seed=seed).sample().reshape((self.N, 3))\n\n\n    def is_valid(self, shallow=True):\n        \"\"\" True if there's no overlapping \"\"\"\n        return (shallow and self.disallow_overlap) or space2MultiPolygon(self.squares).is_valid\n\n    @property\n    def overlap_area(self):\n        if self.disallow_overlap:\n            return 0\n\n        area = 0\n        # for i, square1 in enumerate(self.squares.geoms):\n            # for square2 in list(self.squares.geoms)[i+1:]:\n        for square1, square2 in itertools.combinations(self.squares, 2):\n            area += Polygon(compute_corners(square1)).intersection(Polygon(compute_corners(square2))).area\n        return area\n\n    def min_rotated_rect_extents(self, side_len=1) -&gt; Tuple['minx', 'miny', 'maxx', 'maxy']:\n        corners = compute_all_corners(self.squares)\n        xs = corners[:,:,0]\n        ys = corners[:,:,1]\n        return np.min(xs), np.min(ys), np.max(xs), np.max(ys)\n\n    @property\n    def side_len(self):\n        # minx = np.min(self.squares[])\n        # x, y = self.squares.minimum_rotated_rectangle.exterior.coords.xy\n        minx, miny, maxx, maxy = self.min_rotated_rect_extents()\n        return max(maxx - minx, maxy - miny)\n        # edge_length = (Point(x[0], y[0]).distance(Point(x[1], y[1])), Point(x[1], y[1]).distance(Point(x[2], y[2])))\n        # return max(edge_length)\n\n    @property\n    def wasted_space(self):\n        return self.side_len**2 - self.N\n\n    @property\n    def within_boundary(self):\n        if not self.boundary:\n            return True\n        for x, y, rot in self.squares:\n            if (x &lt; self.boundary or\n                y &lt; self.boundary or\n                abs(x - self.search_space) &lt; self.boundary or\n                abs(y - self.search_space) &lt; self.boundary\n            ): return False\n        return True\n\n\n    # def render_matplotlib(self):\n        # plt.gca().set_aspect('equal')\n        # for geom in self.squares.geoms:\n        #     xs, ys = geom.exterior.xy\n        #     plt.fill(xs, ys, alpha=0.5, fc='r', ec='none')\n        # plt.show()\n\n    def render_shapely(self):\n        # self._display_id.update(space2MultiPolygon(self.squares))\n        display(space2MultiPolygon(self.squares), clear=True)\n\n    def render_pygame(self):\n        scaled_squares = self.squares.copy()\n        scaled_squares[:,:2] *= self.scale\n        scaled_squares[:,:2] += self.offset\n\n        # Draw all the polygons\n        for square in compute_all_corners(scaled_squares, side_len=self.scale):\n            # print(square)\n            pygame.draw.polygon(self.surf, (200, 45, 45, 175), square)\n\n        # Draw the bounding box of the squares\n        minx, miny, maxx, maxy = self.min_rotated_rect_extents(side_len=self.scale)\n        corners = np.array([\n            [minx, miny],\n            [minx, maxy],\n            [maxx, maxy],\n            [maxx, miny],\n        ])\n        gfxdraw.polygon(self.surf, (corners*self.scale)+self.offset, (0,0,0))\n        # Draw the search space\n        gfxdraw.rectangle(self.surf, (self.offset, self.offset, self.search_space*self.scale, self.search_space*self.scale), (0,0,0))\n\n        # Draw the boundaries\n        if self.boundary:\n            boundsColor = (210, 95, 79, 100)\n            off = self.offset\n            b = self.boundary * self.scale\n            ss = self.search_space * self.scale\n            # Top\n            gfxdraw.box(self.surf, ((off, off), (ss-b-1, b)), boundsColor)\n            # Right\n            gfxdraw.box(self.surf, ((off+ss, off), (-b, ss-b)), boundsColor)\n            # Left\n            gfxdraw.box(self.surf, ((off+b, off+b), (-b, ss-b)), boundsColor)\n            # Bottom\n            gfxdraw.box(self.surf, ((off+b+1, off+ss-b), (ss-b, b)), boundsColor)"
  },
  {
    "objectID": "posts/SeniorProject/index.html#the-script",
    "href": "posts/SeniorProject/index.html#the-script",
    "title": "Senior Project",
    "section": "The Script",
    "text": "The Script\nThis is the actual script that I used, with annotations\n\nDependancies\nTo set up Tensorflow with hardware acceleration, see this blog post I wrote for this project specifically. All the versions and hardware are what I used for running this project.\n\n\nInstall dependancies\n# Python 3.11.9\n%pip install \\\n    gymnasium==0.29.1 \\\n    numpy==1.26.4 \\\n    tensorflow==2.14.0 \\\n    matplotlib==3.9.0 \\\n    pygame==2.5.2 \\\n    shapely==2.0.4 \\\n    pydot==2.0.0 \\\n    graphviz==0.20.3 \\\n    rich==13.7.1 \\\n    pillow==10.3.0 \\\n    ipykernel==6.29.4\n\n\n\n\nImports\nimport gymnasium as gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras as ks\nimport inspect\nimport random\nimport time\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nimport subprocess\nfrom rich import print as rprint\nfrom IPython.display import HTML, display\nimport base64\nfrom PIL import Image\n\n\n\n\nConfiguration & Enviorment\n\n\nCode\n# Instead of setting the envioronment up as a local package, which seems to be the standard,\n# I just import directly from the file\n\nenv = SquareEnv(\n    # 11 Squares\n    N=11,\n    # This is the size of the area the squares are allowed to be in\n    # This is in \"atomic units\". Pixels, except everything is scaled up a ton so we can see it.\n    search_space=11,\n    render_mode=\"pygame\",\n    # How maximum reate at which we translate and rotate the squares per step\n    shift_rate=.05,\n    rot_rate=.03,\n    # The maximum amount of steps the enviornment can last for, before it resets itself\n    max_steps=1000,\n    # This defines what to do if a squares goes outside the search space:\n    # clip: rotation and position stay the same if they try to go out\n    # loop: rotation and position loop back to the other end if they try to go out\n    # mixed: loop the rotation, but clip the position\n    bound_method='mixed',\n    # Whether we allow the squares to overlap or not\n    # At first, I figured allowing overlap, but having a steep cost to squares overlaping would help\n    # because when the squares get packed too tight, they can just overlap to a different state,\n    # instead of having to back out and come back in, valid all the time\n    # I'm unsure which is better, to be honest, but disallowing overlap made tuning the reward\n    # function a little easier\n    disallow_overlap=True,\n    # The width of the areas on the sides of the search space that decrease the reward for each\n    # square in it\n    boundary=2,\n    # How much area we're allowed to overlap before we reset the episode (in units of \"pixels\"^2\n    # (see above))\n    max_overlap=1,\n    # This defines we we reset the squares.\n    # random: the squares are randomly placed\n    # valid: the squares are randomly placed, but not overlapping\n    # array: the squares are arrayed in a grid, but are randomly \"jiggled\"\n    start_config='array',\n    screen_size=(300, 500),\n)\n\nnum_states = np.product(env.observation_space.shape)\nprint(f\"Size of State Space -&gt; {num_states}\")\nnum_actions = np.product(env.action_space.shape)\nprint(f\"Size of Action Space -&gt; {num_actions}\")\n\nupper_bound = env.action_space.high\nlower_bound = env.action_space.low\n\nprint(f\"Max Value of an Action -&gt;\")\ndisplay(upper_bound)\nprint(f\"Min Value of an Action -&gt;\")\ndisplay(lower_bound)\n\n\nSize of State Space -&gt; 33\nSize of Action Space -&gt; 33\nMax Value of an Action -&gt;\n\n\narray([0.05, 0.05, 0.03, 0.05, 0.05, 0.03, 0.05, 0.05, 0.03, 0.05, 0.05,\n       0.03, 0.05, 0.05, 0.03, 0.05, 0.05, 0.03, 0.05, 0.05, 0.03, 0.05,\n       0.05, 0.03, 0.05, 0.05, 0.03, 0.05, 0.05, 0.03, 0.05, 0.05, 0.03])\n\n\nMin Value of an Action -&gt;\n\n\narray([-0.05, -0.05, -0.03, -0.05, -0.05, -0.03, -0.05, -0.05, -0.03,\n       -0.05, -0.05, -0.03, -0.05, -0.05, -0.03, -0.05, -0.05, -0.03,\n       -0.05, -0.05, -0.03, -0.05, -0.05, -0.03, -0.05, -0.05, -0.03,\n       -0.05, -0.05, -0.03, -0.05, -0.05, -0.03])\n\n\n\n\nDDPG Buffer\nThis is the code buffer. It’s the main engine of the DDPG algorithm. It’s minorly modified from some place on the internet I can’t find anymore. I used to be linked to this tutorial, which is where I learned the DDPG concepts from\n\n\nBuffer\nclass Buffer:\n    def __init__(self, buffer_capacity=100000, batch_size=64):\n        # Number of \"experiences\" to store at max\n        self.buffer_capacity = buffer_capacity\n        # Num of tuples to train on.\n        self.batch_size = batch_size\n\n        # Its tells us num of times record() was called.\n        self.buffer_counter = 0\n\n        # Instead of list of tuples as the exp.replay concept go\n        # We use different np.arrays for each tuple element\n        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n\n    # Takes (s,a,r,s') obervation tuple as input\n    def record(self, state, action, reward, next_state):\n        # Set index to zero if buffer_capacity is exceeded, replacing old records\n        index = self.buffer_counter % self.buffer_capacity\n\n        self.state_buffer[index] = state\n        self.action_buffer[index] = action\n        self.reward_buffer[index] = reward\n        self.next_state_buffer[index] = next_state\n\n        self.buffer_counter += 1\n\n    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n    # TensorFlow to build a static graph out of the logic and computations in our function.\n    # This provides a large speed up for blocks of code that contain many small TensorFlow\n    # operations such as this one.\n    @tf.function\n    def update(self, state_batch, action_batch, reward_batch, next_state_batch):\n        # print('update called')\n        # Training and updating Actor & Critic networks.\n        with tf.GradientTape() as tape:\n            # What we think we should do\n            target_actions = target_actor(next_state_batch, training=True)\n            # How good we think that is (the target Q-Network)\n            target_evaluation = target_critic([next_state_batch, target_actions], training=True)\n            y = reward_batch + gamma * target_evaluation\n            critic_value = critic_model([state_batch, action_batch], training=True)\n            # Get the average amount we were off by in predicting how well we would do, squared,\n            # and that's the loss\n            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n\n        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n        # Added Gradient clipping on this line\n        clipped_grad = [tf.clip_by_value(grad, -1, 1) for grad in critic_grad]\n        critic_optimizer.apply_gradients(zip(clipped_grad, critic_model.trainable_variables))\n\n        with tf.GradientTape() as tape:\n            actions = actor_model(state_batch, training=True)\n            critic_value = critic_model([state_batch, actions], training=True)\n            # Used `-value` as we want to maximize the value given by the critic for our actions\n            actor_loss = -tf.math.reduce_mean(critic_value)\n\n        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n        actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n\n        return actor_loss, critic_loss\n\n    # We compute the loss and update parameters\n    def learn(self):\n        # Get sampling range\n        record_range = min(self.buffer_counter, self.buffer_capacity)\n        # Randomly sample indices\n        batch_indices = np.random.choice(record_range, self.batch_size)\n\n        # Convert to tensors\n        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n\n        return self.update(state_batch, action_batch, reward_batch, next_state_batch)\n\n# This updates target parameters slowly\n# Based on rate `tau`, which is much less than one.\n@tf.function\ndef update_target(target_weights, weights, tau):\n    for (a, b) in zip(target_weights, weights):\n        a.assign(b * tau + a * (1 - tau))\n\n\n\n\nDefine Actor & Critic Models\nThese are the main actor and critic neural nets\n\n\nCode\n# Tells a later cell to reset the weights (since we might have changed how we initialize them in\n# this cell)\nINIT_WEIGHTS = False\n\nreg_str = .01\ndef get_actor():\n    # Initialize weights\n    # last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n    # last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n\n    inputs = layers.Input(shape=(num_states,))\n    out = layers.Dense(750, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(inputs)\n    out = layers.Dense(1024, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(out)\n    # I expiremented with making this layer have an exponential activation function\n    out = layers.Dense(512, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(out)\n    out = layers.Dense(64, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(out)\n    out = layers.Dense(256, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(out)\n    # outputs = layers.Dense(num_actions, activation='softsign', kernel_initializer=last_init)(out)\n    outputs = layers.Dense(num_actions, activation='tanh')(out)\n\n    # This is rescale the output from between -1 - 1 to the appropriate action space scale\n    outputs = outputs * upper_bound\n    model = tf.keras.Model(inputs, outputs)\n    return model\n\ndef get_critic():\n    # State as input\n    state_input = layers.Input(shape=(num_states,))\n    state_out = layers.Dense(16, activation=\"tanh\")(state_input)\n    state_out = layers.Dense(32, activation=\"tanh\")(state_out)\n\n    # Action as input\n    action_input = layers.Input(shape=(num_actions,))\n    action_out = layers.Dense(32, activation=\"tanh\")(action_input)\n\n    # Both are passed through seperate layer before concatenating\n    concat = layers.Concatenate()([state_out, action_out])\n\n    out = layers.Dense(1024, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(concat)\n    out = layers.Dense(512, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(concat)\n    out = layers.Dense(64, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(concat)\n    out = layers.Dense(256, activation=\"tanh\", kernel_regularizer=ks.regularizers.l2(reg_str))(out)\n    outputs = layers.Dense(1)(out)\n\n    # Outputs single value for give state-action\n    model = tf.keras.Model([state_input, action_input], outputs)\n    return model\n\n\n\n\nWeights Handling\nactor_weights_file = 'actor_weights'\ncritic_weights_file = 'critic_weights'\ntarget_actor_weights_file = 'target_actor_weights'\ntarget_critic_weights_file = 'target_critic_weights'\nload_weights = False\nload_target_weights = False\n\n\nInitialize the actor and critic models\n\n\nCode\nactor_model = get_actor()\ncritic_model = get_critic()\n\ntarget_actor = get_actor()\ntarget_critic = get_critic()\n\n\n\n\nMore Weights Handling\nif load_weights:\n    actor_model.load_weights(actor_weights_file)\n    critic_model.load_weights(critic_weights_file)\n\nif load_target_weights:\n    target_actor.load_weights(target_actor_weights_file)\n    target_critic.load_weights(target_critic_weights_file)\nelse:\n    # Making the weights equal initially\n    target_actor.set_weights(actor_model.get_weights())\n    target_critic.set_weights(critic_model.get_weights())\n\n\n\n\nHyperparameters\nThese are some of the easily tunable hyperparameters\n\n\nCode\n\n# Learning rate for actor-critic models\n# Previously: .002\ncritic_lr = 0.0005\n# Previously: .0015\nactor_lr = 0.0003\n\ncritic_optimizer = tf.keras.optimizers.Adam(critic_lr)\nactor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n\n# How many resets we're running for.\ntotal_episodes = 1000\n# Discount factor for future rewards\npatience = gamma = 0.99\n# Used to update target networks\ntau = 0.005\n\navg_reward_episodes = 40\n\n# This used to be a custom OUActionNoise class, but then a paper came out saying that\n# gaussian noise works just as well\nstd_dev = 0.2\nnoise = lambda mu=0, std_dev=std_dev: random.gauss(mu, std_dev)\nadd_noise = True\nnoise_cooldown = actor_lr * 4\n\nbuffer = Buffer(10000, batch_size=64)\n\nreset_weights = False\nreset_weights_once = True\n\n\n\n\nLoad Weights\nif not INIT_WEIGHTS:\n    actor_model.save_weights('actor_model_init.h5')\n    critic_model.save_weights('critic_model_init.h5')\n    target_actor.save_weights('target_actor_init.h5')\n    target_critic.save_weights('target_critic_init.h5')\n    INIT_WEIGHTS = True\n\n\n\n\nShow our configuration in the window\nenv.show_strings = [\n    'ML Config:',\n    f'gamma: {gamma}',\n    f'tau: {tau}',\n    f'Buffer Capacity: {buffer.buffer_capacity}',\n    f'Buffer Batch Size: {buffer.batch_size}',\n    # f'avg of prev: {avg_reward_episodes}',\n    '-------------------------',\n    'Env Config:',\n    f'Number of Squares: {env.N}',\n    f'Max Steps: {env.max_steps}',\n    f'Max Overlap: {env.max_overlap}',\n    f'Shift Rate: {env.shift_rate}',\n    f'Rotation Rate: {env.rot_rate}',\n    f'Start Config: {env.start_config}',\n    f'Boundary: {env.boundary}',\n    f'Bound Method: {env.bound_method}',\n    '-------------------------',\n]\n\n\n\n\nA function to save all the information from the runs\ndef save_output(captured, run_name):\n    dir = Path('runs') / run_name\n    (dir / 'weights').mkdir(exist_ok=True, parents=True)\n\n    # Save the outputs\n    names = (\n        'config.html',\n        'EpisodeRewardGraph.png',\n        'LossesGraph.png',\n        'BestState.svg',\n        'actor_pretty.png',\n        'critic_pretty.png',\n    )\n    for output, name in zip(captured.outputs, names):\n        with open(dir / name, 'wb') as f:\n            if 'text/html' in output.data:\n                f.write(str(output.data['text/html']).encode())\n            if 'image/png' in output.data:\n                f.write(base64.decodebytes(bytes(output.data['image/png'], \"utf-8\")))\n            if 'image/svg+xml' in output.data:\n                f.write(output.data['image/svg+xml'].encode())\n\n    with (dir / 'best.txt').open('w') as f:\n        f.write(captured.stdout)\n\n    with (dir / 'reward_func.py').open('w') as f:\n        f.write(inspect.getsource(env._get_reward))\n\n    # Save the weights\n    actor_model.save_weights(dir / 'weights' / 'actor_weights')\n    critic_model.save_weights(dir / 'weights' / 'critic_weights')\n\n    target_actor.save_weights(dir / 'weights' / 'target_actor_weights')\n    target_critic.save_weights(dir / 'weights' / 'target_critic_weights')\ndisp = display('', display_id=True)\n\n\n\n\nMain Loop\nThis is the loop that actually runs the RL agent in the enviornment\nUsing the cell above and some cell magic, this logs everything to a directory, so all the information is saved and reproducible\n\n\nCode\n%%capture run\n#! REMEMBER: Don't forget to increment the run name\n\nif reset_weights or reset_weights_once:\n    actor_model.load_weights('actor_model_init.h5')\n    critic_model.load_weights('critic_model_init.h5')\n    target_actor.load_weights('target_actor_init.h5')\n    target_critic.load_weights('target_critic_init.h5')\n    reset_weights_once = False\n\n# To store reward history of each episode (for plotting)\nep_reward_list = []\n# To store average reward history of last few episodes (for plotting)\navg_reward_list = []\n\n# These aren't used in the algorithm, they're just so I can make graphs to see what is going on\nep_len_list = [] # To store the length of of each episode\nep_critic_loss = []\nep_actor_loss = []\nstart_time = time.time_ns()\nbest_state_yet = None\nbest_reward_yet = -10000000000\n\ntry:\n    # Run through `total_episodes` number of enviorment resets\n    for ep in range(total_episodes):\n        # Reset the enviornment\n        prev_state, _ = env.reset()\n\n        # The env.search_space here (and after step()) is to normalize the values to within 0-1 so\n        # the NN can interpret them\n        # Note that this assumes the search_space is greater than pi/2 (which shouldn't be a problem)\n        prev_state = np.array([prev_state])/env.search_space\n\n        # Reset all the episodic variables\n        episodic_reward = 0\n        step_count = 0\n        sum_actor_loss = 0\n        sum_critic_loss = 0\n\n        # Run/step through a single episodes\n        while True:\n            step_count += 1\n\n            # Show the enviornment\n            env.render()\n\n            # This is the policy -- deciding what action to take\n            # Get the main actor output (i.e. \"which action do I think we should take?\")\n            sampled_actions = tf.squeeze(actor_model(prev_state)).numpy()\n\n            if add_noise:\n                # This should make the noise fade out over time (proportional to the actor learning\n                # rate)\n                # We want to fade the noise over time, *and* as we step through specific episodes\n                total_cooldown   = max(std_dev - (ep  * noise_cooldown), 0)\n                episode_cooldown = max(std_dev - (step_count * noise_cooldown), 0)\n                sampled_actions += noise((total_cooldown + episode_cooldown) / 2)\n\n            # Make sure the action is in the action space\n            action = np.clip(sampled_actions, lower_bound, upper_bound)\n\n            # Get the state and reward from environment\n            state, reward, done, _, info = env.step(action)\n            state = np.array([state])/env.search_space\n\n            # Now stick the obvervation in the buffer so it can learn from it\n            buffer.record(prev_state, action, reward, state)\n\n            # This is where the Bellman equation is implemented\n            actor_loss, critic_loss = buffer.learn()\n\n            # Now update the NNs with the losses we just calculated\n            update_target(target_actor.variables, actor_model.variables, tau)\n            update_target(target_critic.variables, critic_model.variables, tau)\n\n            # Update the episodic variables\n            episodic_reward += reward\n            sum_actor_loss += actor_loss\n            sum_critic_loss += critic_loss\n            if best_reward_yet &lt; reward:\n                best_state_yet = state\n                best_reward_yet = reward\n\n            # Print a bunch of information to the enviornment window about this step\n            # We have to index these, it doesn't matter with what, because that's how SimpleGym works\n            # It's very similar to why React components all need unique keys. Same concept.\n            env.print['dd'] = 'Episode Stats'\n            env.print['step'] = f'Step: {step_count:.0f}'\n            env.print['reward'] = f'Reward: {reward:.0f}'\n            env.print['avg reward'] = f'Avg. Reward for This Episode: {episodic_reward/step_count:.0f}'\n            env.print['overlap'] = f'Overlap: {info[\"overlap\"]:.1f}'\n            env.print['len'] = f'Side Length: {info[\"len\"]:.1f}'\n            env.print['wasted'] = f'Wasted Space: {info[\"wasted\"]:.1f}'\n            env.display(disp)\n\n            if done: break\n\n            prev_state = state\n\n        # Episode is done, now do some calculations\n        # These are all just for plotting, not actually important to the algorithm\n        ep_reward_list.append(episodic_reward)\n        avg_reward = np.mean(ep_reward_list[-avg_reward_episodes:]) # Mean of last `avg_reward_episodes` episodes\n        avg_reward_list.append(avg_reward)\n        ep_actor_loss.append(sum_actor_loss / step_count)\n        ep_critic_loss.append(sum_critic_loss / step_count)\n        ep_len_list.append(step_count)\n\n        # Print a bunch of information to the enviornment window about this episode\n        env.print['--'] = '-'*20\n        env.print['ddd'] = 'Run Stats'\n        env.print['e'] = f'Episode {env.reset_count}/{total_episodes}'\n        env.print['f'] = f'Last Episode lasted {step_count} steps'\n        env.print['g'] = f'Last Episode Avg. Reward: {episodic_reward/step_count:.1f}'\n        env.print['h'] = f'Avg. Reward of the last {avg_reward_episodes} episodes: {avg_reward:.1f}'\n        env.print['-'] = '-'*20\n        env.print['avg steps'] = f'Avg. Steps / Episode: {np.mean(ep_len_list) :.1f}'\n        env.print['i'] = f'{((time.time_ns() - start_time) / 1000_000_000) / (ep+1):.1f}s / Episode'\n        env.print['j'] = f'{((time.time_ns() - start_time)/1000_000) / sum(ep_len_list):.1f}ms / step'\n\nfinally:\n    # Once we're finished with the simulation, clean up, make some graphs, and log everything we did\n    env.close()\n\n    if best_reward_yet is not None: # If this is the case, there was an error somewhere\n        env.display()\n        fig, ax1 = plt.subplots()\n\n        ax1.plot(avg_reward_list, label='Avg. Reward', color='blue')\n        ax1.set_xlabel(\"Episode\")\n        ax1.set_ylabel(\"Avg. Episodic Reward\", color='blue')\n        ax1.tick_params(axis='y', labelcolor='blue')\n        ax1.axhline(y=0, linestyle='-')\n\n        ax2 = ax1.twinx()\n        ax2.plot(ep_len_list, label='Episode Length', color='green', alpha=.2)\n        ax2.set_ylabel(\"Episode Length\", color='green')\n        ax2.tick_params(axis='y', labelcolor='green')\n\n        fig.tight_layout()\n        plt.show()\n\n        # Losses\n        fig, ax1 = plt.subplots()\n        ax1.plot(ep_critic_loss, label='Avg Critic Loss', color='red')\n        ax2 = ax1.twinx()\n        ax2.plot(ep_actor_loss, label='Avg Actor Loss', color='orange')\n        ax1.legend()\n        ax2.legend(loc=(.01, .8))\n        plt.show()\n\n        # These don't actually print, but get captured and put into the run log\n        # Print the numbers for the best state yet\n        print('Best Reward yet: ', best_reward_yet)\n        print('Best State:')\n        print(best_state_yet)\n\n        # Show the best state yet\n        # I have no idea why this needs to be rescaled. It shouldn't, as far as I can tell\n        # But it's not super relevant. It just means we can't trust it to say whether we're\n        # overlapping or not\n        display(space2MultiPolygon(best_state_yet.reshape((env.N,3)), .1))\n\n        # Show the models we used\n        # The target models are identical to these\n        display(ks.utils.plot_model(actor_model,   show_layer_activations=True, show_shapes=True))\n        display(ks.utils.plot_model(critic_model,  show_layer_activations=True, show_shapes=True))\n\n        # This saves the log to a directory named this\n        save_output(run, 'run9')\n\n\n\n\nExample Log\nHere is part of the log of the last run I did\n\n\nDisplay Log\ndisplay(clear=True)\nrun()\n\n\nBest Reward yet:  423.74475850765157\nBest State:\n[[0.48985256 0.49015426 0.03719943 0.67339533 0.48970593 0.06476117\n  0.85326362 0.49111193 0.0091013  0.97956313 0.5251773  0.01241833\n  0.42890112 0.70183372 0.         0.61079882 0.60942355 0.08382925\n  0.78974631 0.76345304 0.14279967 0.99174527 0.67113802 0.0109104\n  0.42903404 0.79361329 0.14279967 0.691021   0.69134502 0.00892738\n  0.91995521 0.91959155 0.03349545]]\n\n\nML Config:gamma: 0.99tau: 0.005Buffer Capacity: 10000Buffer Batch Size: 64-------------------------Env Config:Number of Squares: 11Max Steps: 1000Max Overlap: 1Shift Rate: 0.03Rotation Rate: 0.02Start Config: arrayBoundary: 2Bound Method: mixed-------------------------Episode StatsStep: 457Reward: 60Avg. Reward for This Episode: 98Overlap: 0.0Side Length: 8.2Wasted Space: 56.0--------------------Run StatsEpisode 1000/1000Last Episode lasted 457 stepsLast Episode Avg. Reward: 98.0Avg. Reward of the last 40 episodes: 29839.6--------------------Avg. Steps / Episode: 361.23.8s / Episode10.5ms / step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave the weights outside of the log\nactor_model.save_weights(actor_weights_file)\ncritic_model.save_weights(critic_weights_file)\n\ntarget_actor.save_weights(target_actor_weights_file)\ntarget_critic.save_weights(target_critic_weights_file)"
  },
  {
    "objectID": "posts/SeniorProject/index.html#summary",
    "href": "posts/SeniorProject/index.html#summary",
    "title": "Senior Project",
    "section": "Summary",
    "text": "Summary\nI wasn’t able to find a better configuration than the best known (which isn’t super surprising). I didn’t actually end up seeing much progress. Having trained a couple DDPG’s before, I know usually you start at least seeing progress after a couple hundred episodes of training. This problem is much harder than the ones I’ve worked on before, but I didn’t see really any progress after 500 episodes.\nYou can see the specific hyperparameters and configurations I tried (on GitHub)[https://github.com/smartycope/SquarePacking/tree/master/runs]\nWhen you try a bunch of things, and none of them seem to work at all, it becomes hard to know what to try next.\n\nMy Guesses as to What is Wrong\n\nI overlooked some simple or not-so-simple flaw in the algorithm I set up, and it’s not training because I set it up wrong\n\nSolutions:\n\nHave someone who knows much more than I do about RL algorithms go over my code with a fine-toothed comb and implement what they suggest\n\n\nMy observation space is too “abstract” (for lack of a better word), and doesn’t adequately encode the positional relationships between the squares\n\nSolutions:\n\nMake the observation space multi-dimentional (say, a shape of (11, 3) instead of (33,)), and try adding a convolutional layer as the first layer of the actor network\nChange the observation space to be a screen, and add a few convolutional layers as the first layers of the actor network\n\n\nThe problem is fundementally too difficult to be solved with a DDPG\n\nSolutions:\n\nSwitch to a different kind of ML architecture, like a transformer network\nTry a different kind of heuristic algorithm…\n\n\n\n\n\nA Different Kind of Hueristic Algorithm\nIf I had more time, and was willing to scrap everything and start over, I would try a more monte carlo/simulated annealing approach. Something like this:\n\nrandomly make discrete movements/rotations of rot_rate, shift_rate\ndecide if it’s better or not according to my reward function\nif it’s better, keep going, if it’s worse, undo it some of the time, and keep it anyway some of the time (r)\n\nThen, have rot_rate, shift_rate, and r decrease slowly as time goes on, to slowly anneal to a more optimal configuration.\nCredit where it’s due, the idea for this was inspired by this YouTube video on algorithmic gerrymandering\n\nI did it\nI ended up implementing that algorithm after all. Here it is, I’ll only go into detail about the parts that I haven’t discussed above. I used the same environment and hardware to run it as I did above.\nI added cooldown to the variables rand (the chance that we make a bad move), rot (the max amount of rotation we can make in a single step), and shift (the max amount of translation we can make in a single step). You can see the graph below for the rates at which they decrease.\nInspired by the above YouTube video, I tried adding an overlap phase. My thinking was that after it starts to approach a local minimum, a lower minimum may be blocked by say, the corner of a different square. If it could just get over there, it would be able to optimize further. This didn’t end up working, and seemed to just make squares overlap, which reduced the reward and had a hard time getting back.\nNote that this portion is independantly runnable, but you will need to run the Dependant Code section first for it to work.\n\n\nImports\nimport numpy as np\nfrom random import randint\nimport random\nimport time\nimport matplotlib.pyplot as plt\nimport itertools as it\n# from Cope import percent\n# from SquareEnv2 import space2MultiPolygon, SquareEnv\n\n\n\n\n\nCode\n\nReward Function\n\n# Modify the reward function\ndef _get_reward(self):\n    # We generally prefer living longer\n    score = 0 # Linear\n    centered_importance = .5 # Exponential\n\n    score -= self.side_len\n\n    # We don't like it when they overlap at all\n    if self.overlap_area &gt; 0:\n        score -= 10\n        # We also want to disincentivize overlapping more, so we can tell if we're starting to\n        # un-overlap (which is better than overlapping more)\n        score -= self.overlap_area\n\n    # I don't want them to just push up against the edges\n    if centered_importance:\n        for x, y, _rot in self.squares:\n            # Remove boundary badness entirely for optimization\n\n            # We want the squares to be close to the center\n            if centered_importance:\n                score -= dist([x, y], [self.search_space / 2, self.search_space / 2]) * centered_importance\n\n    return score\n\n\nSquareEnv._get_reward = _get_reward\n\n\n\n\n\nmodify the environment terminated function\ndef _get_terminated(self):\n    # Optimal: 3.789, best known: 3.877084\n    # There's no overlapping and we're better than the previous best\n    if self.N == 11 and self.side_len &lt; 3.877084 and self.is_valid():\n        print('Holy cow, we did it!!!')\n        print('Coordinates & Rotations:')\n        print(self.squares)\n        with open('~/SQUARE_PARAMETERS.txt', 'w') as f:\n            f.write(str(self.squares))\n        return True\n\n    # If we're almost entirely overlapping, just kill it\n    if self.overlap_area &gt; self.max_overlap:\n        return True\n\n    # Remove the deadlock detection, since it breaks when we only move 1 square at a time\n\n    return False\n\nSquareEnv._get_terminated = _get_terminated\n\n\n\n\n\nCode\n\nConfiguration & Enviorment\n\nenv = SquareEnv(\n    N=11,\n    # It can handle a smaller search space, it speeds things up a little\n    search_space=5.5,\n    render_mode=\"pygame\",\n    # These are the upper bounds\n    shift_rate=.05,\n    rot_rate=.03,\n    max_steps=100_000,\n    bound_method='mixed',\n    # This changes later\n    disallow_overlap=True,\n    boundary=0,\n    # Arbitrarily high overlap: to allow for an \"overlap phase\" (see below)\n    max_overlap=5,\n    # I tried both `valid` and `array`\n    start_config='valid',\n    screen_size=(300, 400),\n)\n\n\n\n\n\n\nCode\n\nHyperparameters\n\n# How many resets we're running for.\ntotal_episodes = 1\n\n# The starting constants\nrand = .5 # Percent\n# The starting rates will always be the highest\nrot = env.rot_rate\nshift = env.shift_rate\n\n# The smallest they're allowed to get\n_min_rand = .01 # percent\n_min_rot = .0001\n_min_shift = .0001\n\n# How much we allow the probability to be modified, based on the score difference\nrand_modify_limit = lambda rand: rand * .5\n\n# The steps at which each phase starts and ends\n# This is disabled at the moment: I found it tended to hinder more than help\noverlap_phase = (env.max_steps * 2, -1)\n\n# Specify a manual rand value while in the overlap_phase (to encourage exploration)\noverlap_phase_rand = rand\n\n# The cooldown functions\n# These specify how we want to \"cooldown\" or decrease the values rand, rot, and shift over steps\n# There's a lot of oppritunity for tuning these functions. In particular, I want to expirement with\n# rot and shift functions that are \"hilly\", and increase and decrease the movement rates cyclicly.\ncooldown = True\nrand_func  = lambda step: (\n    overlap_phase_rand\n    if step &gt; overlap_phase[0] and step &lt; overlap_phase[1]\n    else np.clip(-.000003*step  + rand,  _min_rand, rand)\n)\nrot_func   = lambda step: np.clip(-.0000002*step + rot,   _min_rot, rot)\nshift_func = lambda step: np.clip(-.0000003*step + shift, _min_shift, shift)\n\n\n\n\n\nMake a graph of the cooldown rates\nx = np.array(range(1, env.max_steps if env.max_steps &gt; 1 else 100_000))\nplt.plot(x, list(map(rand_func, x)), label='random cooldown')\nplt.plot(x, rot_func(x), label='rotation cooldown')\nplt.plot(x, shift_func(x), label='shift cooldown')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow our configuration in the window\nenv.show_strings = [\n    '-------------------------',\n    'Env Config:',\n    f'Number of Squares: {env.N}',\n    f'Max Steps: {env.max_steps}',\n    f'Max Overlap: {env.max_overlap}',\n    f'Shift Rate: {env.shift_rate}',\n    f'Rotation Rate: {env.rot_rate}',\n    f'Start Config: {env.start_config}',\n    f'Boundary: {env.boundary}',\n    f'Bound Method: {env.bound_method}',\n    '-------------------------',\n]\n\n\n\n\n\nCode\n\nMain Loop\n\n# To store reward history of each episode (for plotting)\nreward_lists = []\noverlap_lists = []\nbest_state_yet = None\nbest_reward_yet = -10000000000\nbest_side_len_yet = 11\n# Ensure this is set at the beginning of each new run\nenv.disallow_overlap = True\n\ntry:\n    # Run through `total_episodes` number of enviorment resets\n    for ep in range(total_episodes):\n        prev_state, _ = env.reset()\n        prev_reward = env._get_reward()\n        episodic_reward = 0\n        reward_lists.append([])\n        overlap_lists.append([0])\n        start_time = time.time_ns()\n\n        # Run/step through a single episodes\n        for step in it.count(1):\n            env.render()\n            # Calculate the cooldown values for this step\n            # Underscored variables here indicate variables that get reset each step\n            _rand = rand_func(step) if cooldown else rand\n            _shift = shift_func(step) if cooldown else shift\n            _rot = rot_func(step) if cooldown else rot\n            _overlap_phase = step &gt; overlap_phase[0] and step &lt; overlap_phase[1]\n            _rand_limit = rand_modify_limit(_rand)\n\n            # Allow overlap, if we're in the overlap phase\n            if _overlap_phase:\n                env.disallow_overlap = False\n\n            # Make a random action\n            # Only move one square at a time: if we move all at the same time, then 3 of them might\n            # be really good moves, but one is a bad move that invalidates all of them.\n            action = np.zeros((env.N, 3))\n            action[randint(0, env.N-1)] = [\n                # TODO: Next, I want to try expirmenting with other statistical distrobutions\n                random.uniform(-_shift, _shift),\n                random.uniform(-_shift, _shift),\n                random.uniform(-_rot, _rot),\n            ]\n            # TODO: optimization: we're flattening here to then immediately unflatten in the environment _step method\n            action = action.flatten()\n\n            # Recieve state and reward from environment\n            state, reward, done, _, info = env.step(action)\n\n            # Modify the chance of accepting the change based on the amount of reward difference: but limit to between\n            # +/- _rand_limit, since a negative percentage doesn't make much sense, and we still want some randomness\n            _chance = _rand + np.clip((prev_reward - reward) * 10, -_rand_limit, _rand_limit)\n\n            # If we're overlapping more than we were before, minimize the odds that we'll keep the movement\n            # NOTE: We can't set _chance to 0 here, or else once we overlap during the overlap phase, we won't be able\n            # to get out\n            if not _overlap_phase and info['overlap'] &gt; overlap_lists[ep][-1]:\n                _chance = _rand - _rand_limit\n\n            if (\n                (\n                    # If we're overlaping at all, and it's not the overlap phase, or if our current state is worse than\n                    # the one before we made the move, undo the move we just took `_chance`% of the time.\n                    # We don't want to do it *all* the time, because we'd quickly only find the local minumum.\n                    # This is the premise of simulated annealing\n                    (info['overlap'] &gt; 0 and not _overlap_phase)\n                    or\n                    reward &lt; prev_reward\n                )\n                and not percent(_chance)\n            ):\n                # Step back - undo what we just did\n                state, reward, done, _, info = env.step(-action)\n\n            # If we've finished the overlap phase, and we've gotten to a point where we're no longer overlapping,\n            # disable overlap again\n            if not env.disallow_overlap and not info['overlap']:\n                env.disallow_overlap = True\n\n            # ------------------ Not part of the algorithm --------------------\n            reward_lists[ep].append(reward)\n            overlap_lists[ep].append(info['overlap'])\n\n            if best_side_len_yet &gt; info['len']:\n                best_state_yet = state\n                best_side_len_yet = info['len']\n                best_reward_yet = reward\n\n            episodic_reward += reward\n            env.print['e'] = f'Episode {env.reset_count}/{total_episodes}:'\n            env.print['step'] = f'Step: {step:.0f}'\n            env.print['reward'] = f'Reward: {reward:.3f}'\n            env.print['overlap'] = f'Overlap: {info[\"overlap\"]:.5f}'\n            env.print['len'] = f'Side Length: {info[\"len\"]:.1f}'\n            env.print['wasted'] = f'Wasted Space: {info[\"wasted\"]:.1f}'\n            env.print['_rand'] = f'annealing coeff: {_rand:.5f}'\n            env.print['_shift'] = f'shift rate: {_shift:.5f}'\n            env.print['_rot'] = f'rotation rate: {_rot:.5f}'\n            env.print['avg reward'] = f'Avg. Reward for This Episode: {episodic_reward/step:.2f}'\n            env.print['phase'] = f\"Phase: {'Overlap' if _overlap_phase else 'Main'}\"\n            env.print['eee'] = f'_rand: {_chance:.2%}'\n            env.print['j'] = f'{((time.time_ns() - start_time)/1000_000) / step:.2f}ms / step'\n\n            if done: break\n\n            prev_state = state\n            prev_reward = reward\n\nfinally:\n    env.close()\n    if best_reward_yet is not None:\n        # Make the graph, and display some of the output\n        env.display()\n\n        fig, ax1 = plt.subplots()\n\n        for ep, i in enumerate(reward_lists):\n            ax1.plot(i, label=f'Episode {ep} reward')\n\n        ax2 = ax1.twinx()\n        x = np.array(range(len(max(reward_lists, key=len))))\n        ax2.plot(x, list(map(rand_func, x)), label='random cooldown', alpha=.4)\n        ax2.plot(rot_func(x), label='rotation cooldown', alpha=.4)\n        ax2.plot(shift_func(x), label='shift cooldown', alpha=.4)\n\n        for ep, i in enumerate(overlap_lists):\n            ax2.plot(i, label=f'Episode {ep} overlap')\n\n        ax1.legend()\n        ax2.legend()\n        # ax1.vlines(overlap_phase, -22, -3)\n        plt.show()\n        print('Best side length yet: ', best_side_len_yet)\n        print('Best Reward yet: ', best_reward_yet)\n        print('Best State:')\n        print(best_state_yet)\n\n        display(space2MultiPolygon(best_state_yet.reshape((env.N,3)), 1))\n\n\n\n-------------------------Env Config:Number of Squares: 11Max Steps: 100000Max Overlap: 5Shift Rate: 0.05Rotation Rate: 0.03Start Config: validBoundary: 0Bound Method: mixed-------------------------Episode 1/1:Step: 83921Reward: -11.434Overlap: 0.00000Side Length: 4.3Wasted Space: 7.8annealing coeff: 0.24824shift rate: 0.02482rotation rate: 0.01322Avg. Reward for This Episode: -12.34Phase: Main_rand: 29.67%3.66ms / step\n\n\n\n\n\n\n\n\n\nBest side length yet:  4.2757519302189095\nBest Reward yet:  -11.537943776434863\nBest State:\n[2.33215984 1.64005651 1.23660412 2.85851348 3.68318778 1.35392014\n 0.97938896 3.20995917 1.17055286 4.056265   4.44360427 1.51664717\n 3.90260406 2.22845498 1.22983613 2.92982993 2.53654343 1.21451204\n 1.85830051 4.03639402 1.39096229 3.84755382 3.34973736 1.36236215\n 3.30502829 1.3204964  1.26810013 1.22718773 1.96212431 1.24175536\n 1.93874577 2.88557254 1.15993837]\n\n\n\n\n\n\n\n\n\n\n\n\nThis is what it looks like when it’s running\n\n\nThis is a graph of when I tried using the overlap_phase. In the reward function, I disincentivized overlapping at all, and then additionally disincentivized the amount of overlap. As you can see, once overlap is allowed, it starts doing so, and the reward plumets. I don’t want to stop disincentivizing overlap during the overlap phase, because then it would quickly all collapse to the center. I ended up removing the overlap phase entirely based on this graph, because you can see the reward increasing, and suddenly dropping, and then rising back up to along the same curve as you can see in the graph above.\n\n\n\nWith an overlap phase"
  },
  {
    "objectID": "posts/SeniorProject/index.html#summary-part-2",
    "href": "posts/SeniorProject/index.html#summary-part-2",
    "title": "Senior Project",
    "section": "Summary Part 2",
    "text": "Summary Part 2\nUsing the simulated annealing method ended up working significantly better than the DDPG algorithm. You can actually see it making progress while it’s running, and while the reward values aren’t comparable, the graphs clearly show improvement while using simulated annealing as opposed to the DDPG algorithm.\nI wasn’t able to find a better configuration than the best known, the best I could do was ~4.276. However, I feel like there’s still untapped potential here. I only started on the 2nd algorithm in the last couple of days, and plan to continue to fine-tune it going forward.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguration\nSide length\nImage\n\n\n\n\nOptimal\n3.789\n???\n\n\nBest Known\n3.877\n\n\n\nTrivial\n4.000\n\n\n\nMine\n3.920\n\n\n\nMine\n3.947\n\n\n\nMine\n3.949\n\n\n\nMine\n4.001\n\n\n\n\n\n\n\n\n\n\n\n\nSide Length\n3.920\n3.947\n\n\n\n\nClose to Best Known\n3.920\n-\n\n\nThrees\n\n."
  }
]